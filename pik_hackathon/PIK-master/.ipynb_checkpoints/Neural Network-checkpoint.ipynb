{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from preprocess import preprocess_train, preprocess, FEATURES, CATEGORICAL_FEATURES, TEST_FEATURES, CATEGORICAL_TEST_FEATURES_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y = preprocess_train(train, categotical_features=CATEGORICAL_FEATURES, features=TEST_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y)\n",
    "y = y[:, None]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8716, 1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def encode_categorical(X, cat_feat):\n",
    "    '''\n",
    "    Encodes categorical features with one-hot encoding and adds it into model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy.ndarray\n",
    "        Training features\n",
    "    cat_feat: list of int\n",
    "        Categorical features indices\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tweaked_X: numpy.ndarray\n",
    "        Tweaked X\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # All the rest\n",
    "    rest = np.ones(X.shape[1], np.bool)\n",
    "    rest[cat_feat] = False\n",
    "    \n",
    "    X_rest = X[:, rest]\n",
    "    \n",
    "    # Encoded\n",
    "    one_hot_encoded = []\n",
    "    \n",
    "    for col_idx in cat_feat:  \n",
    "        encoded = label_binarize(X[:, col_idx], np.unique(X[:, col_idx]).astype(int))\n",
    "        \n",
    "        #print encoded.shape\n",
    "        \n",
    "        one_hot_encoded.append(\n",
    "            encoded\n",
    "        )\n",
    "    \n",
    "    one_hot_encoded.append(X_rest)\n",
    "    \n",
    "    return np.hstack(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoded_X = encode_categorical(X, CATEGORICAL_TEST_FEATURES_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_X = scaler.fit_transform(encoded_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8716, 44) (8716, 46)\n"
     ]
    }
   ],
   "source": [
    "print X.shape, encoded_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iterator(X, y, batch_size):\n",
    "    for i in range(0, len(X) - batch_size, batch_size):\n",
    "        yield X[i:i+batch_size], y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### CrossVal mean scores\n",
    "* catboost_d10: 224.01548277848525\n",
    "* catboost_d16: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training simple model\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1 Loss:412.766143799 Val loss: 361.678192139\n",
      "Epoch 2 Loss:335.971099854 Val loss: 343.146362305\n",
      "Epoch 3 Loss:327.95111084 Val loss: 339.529602051\n",
      "Epoch 4 Loss:324.552337646 Val loss: 336.568695068\n",
      "Epoch 5 Loss:321.956970215 Val loss: 334.051513672\n",
      "Epoch 6 Loss:319.685577393 Val loss: 331.712432861\n",
      "Epoch 7 Loss:317.505859375 Val loss: 329.424835205\n",
      "Epoch 8 Loss:315.311218262 Val loss: 327.147796631\n",
      "Epoch 9 Loss:313.059204102 Val loss: 324.860107422\n",
      "Epoch 10 Loss:310.759246826 Val loss: 322.552734375\n",
      "Epoch 11 Loss:308.406188965 Val loss: 320.22454834\n",
      "Epoch 12 Loss:306.022705078 Val loss: 317.909667969\n",
      "Epoch 13 Loss:303.66418457 Val loss: 315.656951904\n",
      "Epoch 14 Loss:301.370452881 Val loss: 313.487243652\n",
      "Epoch 15 Loss:299.172943115 Val loss: 311.423034668\n",
      "Epoch 16 Loss:297.09475708 Val loss: 309.491363525\n",
      "Epoch 17 Loss:295.151184082 Val loss: 307.70690918\n",
      "Epoch 18 Loss:293.335510254 Val loss: 306.064849854\n",
      "Epoch 19 Loss:291.645019531 Val loss: 304.567138672\n",
      "Epoch 20 Loss:290.081817627 Val loss: 303.226501465\n",
      "Epoch 21 Loss:288.634429932 Val loss: 302.035675049\n",
      "Epoch 22 Loss:287.291137695 Val loss: 300.984924316\n",
      "Epoch 23 Loss:286.048828125 Val loss: 300.052154541\n",
      "Epoch 24 Loss:284.896148682 Val loss: 299.221008301\n",
      "Epoch 25 Loss:283.825500488 Val loss: 298.476196289\n",
      "Epoch 26 Loss:282.82434082 Val loss: 297.803405762\n",
      "Epoch 27 Loss:281.886566162 Val loss: 297.194671631\n",
      "Epoch 28 Loss:281.004547119 Val loss: 296.644470215\n",
      "Epoch 29 Loss:280.168426514 Val loss: 296.143157959\n",
      "Epoch 30 Loss:279.371246338 Val loss: 295.680267334\n",
      "Epoch 31 Loss:278.60736084 Val loss: 295.24319458\n",
      "Epoch 32 Loss:277.872131348 Val loss: 294.827972412\n",
      "Epoch 33 Loss:277.160217285 Val loss: 294.431488037\n",
      "Epoch 34 Loss:276.467193604 Val loss: 294.053375244\n",
      "Epoch 35 Loss:275.791564941 Val loss: 293.689971924\n",
      "Epoch 36 Loss:275.131530762 Val loss: 293.341094971\n",
      "Epoch 37 Loss:274.484832764 Val loss: 293.006622314\n",
      "Epoch 38 Loss:273.849731445 Val loss: 292.680664062\n",
      "Epoch 39 Loss:273.227905273 Val loss: 292.362640381\n",
      "Epoch 40 Loss:272.617614746 Val loss: 292.05355835\n",
      "Epoch 41 Loss:272.017242432 Val loss: 291.7550354\n",
      "Epoch 42 Loss:271.424957275 Val loss: 291.465759277\n",
      "Epoch 43 Loss:270.839691162 Val loss: 291.185028076\n",
      "Epoch 44 Loss:270.259002686 Val loss: 290.909606934\n",
      "Epoch 45 Loss:269.687072754 Val loss: 290.642486572\n",
      "Epoch 46 Loss:269.126159668 Val loss: 290.381500244\n",
      "Epoch 47 Loss:268.574768066 Val loss: 290.125732422\n",
      "Epoch 48 Loss:268.030487061 Val loss: 289.876922607\n",
      "Epoch 49 Loss:267.492248535 Val loss: 289.631988525\n",
      "Epoch 50 Loss:266.961547852 Val loss: 289.392181396\n",
      "Epoch 51 Loss:266.437835693 Val loss: 289.156463623\n",
      "Epoch 52 Loss:265.920379639 Val loss: 288.925231934\n",
      "Epoch 53 Loss:265.407684326 Val loss: 288.698883057\n",
      "Epoch 54 Loss:264.900360107 Val loss: 288.47644043\n",
      "Epoch 55 Loss:264.398498535 Val loss: 288.258666992\n",
      "Epoch 56 Loss:263.902984619 Val loss: 288.045501709\n",
      "Epoch 57 Loss:263.413238525 Val loss: 287.839935303\n",
      "Epoch 58 Loss:262.928436279 Val loss: 287.639312744\n",
      "Epoch 59 Loss:262.447753906 Val loss: 287.443145752\n",
      "Epoch 60 Loss:261.970977783 Val loss: 287.251403809\n",
      "Epoch 61 Loss:261.498443604 Val loss: 287.062194824\n",
      "Epoch 62 Loss:261.02835083 Val loss: 286.878509521\n",
      "Epoch 63 Loss:260.562164307 Val loss: 286.699707031\n",
      "Epoch 64 Loss:260.098358154 Val loss: 286.525054932\n",
      "Epoch 65 Loss:259.636505127 Val loss: 286.353057861\n",
      "Epoch 66 Loss:259.177398682 Val loss: 286.182098389\n",
      "Epoch 67 Loss:258.720825195 Val loss: 286.013427734\n",
      "Epoch 68 Loss:258.266693115 Val loss: 285.847381592\n",
      "Epoch 69 Loss:257.815216064 Val loss: 285.681365967\n",
      "Epoch 70 Loss:257.365661621 Val loss: 285.516693115\n",
      "Epoch 71 Loss:256.917663574 Val loss: 285.357818604\n",
      "Epoch 72 Loss:256.472259521 Val loss: 285.201690674\n",
      "Epoch 73 Loss:256.028533936 Val loss: 285.049438477\n",
      "Epoch 74 Loss:255.586700439 Val loss: 284.899627686\n",
      "Epoch 75 Loss:255.145629883 Val loss: 284.752929688\n",
      "Epoch 76 Loss:254.705184937 Val loss: 284.608856201\n",
      "Epoch 77 Loss:254.26461792 Val loss: 284.467071533\n",
      "Epoch 78 Loss:253.824356079 Val loss: 284.328613281\n",
      "Epoch 79 Loss:253.384399414 Val loss: 284.194549561\n",
      "Epoch 80 Loss:252.944549561 Val loss: 284.063720703\n",
      "Epoch 81 Loss:252.504989624 Val loss: 283.934997559\n",
      "Epoch 82 Loss:252.06539917 Val loss: 283.810577393\n",
      "Epoch 83 Loss:251.626724243 Val loss: 283.691192627\n",
      "Epoch 84 Loss:251.188842773 Val loss: 283.577697754\n",
      "Epoch 85 Loss:250.750869751 Val loss: 283.470367432\n",
      "Epoch 86 Loss:250.313293457 Val loss: 283.369293213\n",
      "Epoch 87 Loss:249.875534058 Val loss: 283.274871826\n",
      "Epoch 88 Loss:249.438583374 Val loss: 283.185638428\n",
      "Epoch 89 Loss:249.001296997 Val loss: 283.101776123\n",
      "Epoch 90 Loss:248.564590454 Val loss: 283.020812988\n",
      "Epoch 91 Loss:248.128067017 Val loss: 282.947021484\n",
      "Epoch 92 Loss:247.692337036 Val loss: 282.882720947\n",
      "Epoch 93 Loss:247.260498047 Val loss: 282.822601318\n",
      "Epoch 94 Loss:246.831863403 Val loss: 282.762054443\n",
      "Epoch 95 Loss:246.404815674 Val loss: 282.703308105\n",
      "Epoch 96 Loss:245.980682373 Val loss: 282.647613525\n",
      "Epoch 97 Loss:245.558853149 Val loss: 282.596710205\n",
      "Epoch 98 Loss:245.140548706 Val loss: 282.546508789\n",
      "Epoch 99 Loss:244.724411011 Val loss: 282.497497559\n",
      "Epoch 100 Loss:244.310150146 Val loss: 282.451019287\n",
      "Epoch 101 Loss:243.898010254 Val loss: 282.40737915\n",
      "Epoch 102 Loss:243.487731934 Val loss: 282.366882324\n",
      "Epoch 103 Loss:243.080093384 Val loss: 282.327606201\n",
      "Epoch 104 Loss:242.674316406 Val loss: 282.288635254\n",
      "Epoch 105 Loss:242.270706177 Val loss: 282.250610352\n",
      "Epoch 106 Loss:241.868988037 Val loss: 282.212677002\n",
      "Epoch 107 Loss:241.46913147 Val loss: 282.174530029\n",
      "Epoch 108 Loss:241.071365356 Val loss: 282.137298584\n",
      "Epoch 109 Loss:240.675567627 Val loss: 282.100280762\n",
      "Epoch 110 Loss:240.281921387 Val loss: 282.063842773\n",
      "Epoch 111 Loss:239.889755249 Val loss: 282.029785156\n",
      "Epoch 112 Loss:239.498413086 Val loss: 281.996765137\n",
      "Epoch 113 Loss:239.10899353 Val loss: 281.963317871\n",
      "Epoch 114 Loss:238.721221924 Val loss: 281.930236816\n",
      "Epoch 115 Loss:238.334121704 Val loss: 281.898834229\n",
      "Epoch 116 Loss:237.948242188 Val loss: 281.866821289\n",
      "Epoch 117 Loss:237.563369751 Val loss: 281.83493042\n",
      "Epoch 118 Loss:237.180023193 Val loss: 281.804443359\n",
      "Epoch 119 Loss:236.798294067 Val loss: 281.774658203\n",
      "Epoch 120 Loss:236.417678833 Val loss: 281.744781494\n",
      "Epoch 121 Loss:236.03729248 Val loss: 281.717834473\n",
      "Epoch 122 Loss:235.658050537 Val loss: 281.692962646\n",
      "Epoch 123 Loss:235.279693604 Val loss: 281.670440674\n",
      "Epoch 124 Loss:234.90322876 Val loss: 281.649047852\n",
      "Epoch 125 Loss:234.528564453 Val loss: 281.629058838\n",
      "Epoch 126 Loss:234.154937744 Val loss: 281.61151123\n",
      "Epoch 127 Loss:233.783737183 Val loss: 281.595245361\n",
      "Epoch 128 Loss:233.41444397 Val loss: 281.580383301\n",
      "Epoch 129 Loss:233.047470093 Val loss: 281.567108154\n",
      "Epoch 130 Loss:232.682495117 Val loss: 281.556518555\n",
      "Epoch 131 Loss:232.319946289 Val loss: 281.547729492\n",
      "Epoch 132 Loss:231.959472656 Val loss: 281.538665771\n",
      "Epoch 133 Loss:231.600753784 Val loss: 281.528930664\n",
      "Epoch 134 Loss:231.243408203 Val loss: 281.519927979\n",
      "Epoch 135 Loss:230.888366699 Val loss: 281.511688232\n",
      "Epoch 136 Loss:230.534896851 Val loss: 281.504119873\n",
      "Epoch 137 Loss:230.18347168 Val loss: 281.497894287\n",
      "Epoch 138 Loss:229.833587646 Val loss: 281.49319458\n",
      "Epoch 139 Loss:229.484924316 Val loss: 281.488891602\n",
      "Epoch 140 Loss:229.13722229 Val loss: 281.486450195\n",
      "Epoch 141 Loss:228.79107666 Val loss: 281.486999512\n",
      "Epoch 142 Loss:228.447418213 Val loss: 281.489257812\n",
      "Epoch 143 Loss:228.106262207 Val loss: 281.491760254\n",
      "Epoch 144 Loss:227.76763916 Val loss: 281.494995117\n",
      "Epoch 145 Loss:227.431747437 Val loss: 281.498657227\n",
      "Epoch 146 Loss:227.09854126 Val loss: 281.503601074\n",
      "Epoch 147 Loss:226.767349243 Val loss: 281.510498047\n",
      "Epoch 148 Loss:226.43812561 Val loss: 281.518218994\n",
      "Epoch 149 Loss:226.110473633 Val loss: 281.527252197\n",
      "Epoch 150 Loss:225.784439087 Val loss: 281.537109375\n",
      "Epoch 151 Loss:225.459838867 Val loss: 281.547637939\n",
      "Epoch 152 Loss:225.136795044 Val loss: 281.559173584\n",
      "Epoch 153 Loss:224.815185547 Val loss: 281.572631836\n",
      "Epoch 154 Loss:224.495834351 Val loss: 281.585662842\n",
      "Epoch 155 Loss:224.178253174 Val loss: 281.599304199\n",
      "Epoch 156 Loss:223.862823486 Val loss: 281.612579346\n",
      "Epoch 157 Loss:223.549728394 Val loss: 281.625549316\n",
      "Epoch 158 Loss:223.238983154 Val loss: 281.640136719\n",
      "Epoch 159 Loss:222.930328369 Val loss: 281.653625488\n",
      "Epoch 160 Loss:222.62348938 Val loss: 281.665771484\n",
      "Epoch 161 Loss:222.31867981 Val loss: 281.67755127\n",
      "Epoch 162 Loss:222.016052246 Val loss: 281.689453125\n",
      "Epoch 163 Loss:221.715255737 Val loss: 281.700836182\n",
      "Epoch 164 Loss:221.416046143 Val loss: 281.711181641\n",
      "Epoch 165 Loss:221.117614746 Val loss: 281.722106934\n",
      "Epoch 166 Loss:220.820358276 Val loss: 281.733917236\n",
      "Epoch 167 Loss:220.524688721 Val loss: 281.745361328\n",
      "Epoch 168 Loss:220.230010986 Val loss: 281.757476807\n",
      "Epoch 169 Loss:219.936401367 Val loss: 281.769989014\n",
      "Epoch 170 Loss:219.64440918 Val loss: 281.78338623\n",
      "Epoch 171 Loss:219.354049683 Val loss: 281.798675537\n",
      "Epoch 172 Loss:219.065109253 Val loss: 281.815582275\n",
      "Epoch 173 Loss:218.777709961 Val loss: 281.833648682\n",
      "Epoch 174 Loss:218.491546631 Val loss: 281.853027344\n",
      "Epoch 175 Loss:218.206573486 Val loss: 281.874359131\n",
      "Epoch 176 Loss:217.923629761 Val loss: 281.894378662\n",
      "Epoch 177 Loss:217.642440796 Val loss: 281.913116455\n",
      "Epoch 178 Loss:217.363204956 Val loss: 281.930511475\n",
      "Epoch 179 Loss:217.085891724 Val loss: 281.947113037\n",
      "Epoch 180 Loss:216.809967041 Val loss: 281.962432861\n",
      "Epoch 181 Loss:216.535186768 Val loss: 281.977294922\n",
      "Epoch 182 Loss:216.261917114 Val loss: 281.991668701\n",
      "Epoch 183 Loss:215.98916626 Val loss: 282.006408691\n",
      "Epoch 184 Loss:215.717834473 Val loss: 282.021179199\n",
      "Epoch 185 Loss:215.447631836 Val loss: 282.035888672\n",
      "Epoch 186 Loss:215.178924561 Val loss: 282.04989624\n",
      "Epoch 187 Loss:214.912002563 Val loss: 282.061828613\n",
      "Epoch 188 Loss:214.646789551 Val loss: 282.072814941\n",
      "Epoch 189 Loss:214.383575439 Val loss: 282.082489014\n",
      "Epoch 190 Loss:214.121841431 Val loss: 282.092468262\n",
      "Epoch 191 Loss:213.861907959 Val loss: 282.102081299\n",
      "Epoch 192 Loss:213.603485107 Val loss: 282.110565186\n",
      "Epoch 193 Loss:213.346054077 Val loss: 282.11819458\n",
      "Epoch 194 Loss:213.089874268 Val loss: 282.12487793\n",
      "Epoch 195 Loss:212.834686279 Val loss: 282.131744385\n",
      "Epoch 196 Loss:212.580657959 Val loss: 282.138183594\n",
      "Epoch 197 Loss:212.327789307 Val loss: 282.142974854\n",
      "Epoch 198 Loss:212.07585144 Val loss: 282.14666748\n",
      "Epoch 199 Loss:211.825057983 Val loss: 282.151000977\n",
      "Epoch 200 Loss:211.575439453 Val loss: 282.156311035\n",
      "Epoch 201 Loss:211.326660156 Val loss: 282.162963867\n",
      "Epoch 202 Loss:211.07925415 Val loss: 282.170043945\n",
      "Epoch 203 Loss:210.832748413 Val loss: 282.177032471\n",
      "Epoch 204 Loss:210.587371826 Val loss: 282.183227539\n",
      "Epoch 205 Loss:210.342880249 Val loss: 282.189788818\n",
      "Epoch 206 Loss:210.099365234 Val loss: 282.196868896\n",
      "Epoch 207 Loss:209.856506348 Val loss: 282.20489502\n",
      "Epoch 208 Loss:209.614547729 Val loss: 282.214355469\n",
      "Epoch 209 Loss:209.373748779 Val loss: 282.224487305\n",
      "Epoch 210 Loss:209.134017944 Val loss: 282.234649658\n",
      "Epoch 211 Loss:208.894897461 Val loss: 282.244140625\n",
      "Epoch 212 Loss:208.656219482 Val loss: 282.252655029\n",
      "Epoch 213 Loss:208.417633057 Val loss: 282.259094238\n",
      "Epoch 214 Loss:208.18031311 Val loss: 282.265106201\n",
      "Epoch 215 Loss:207.94392395 Val loss: 282.271850586\n",
      "Epoch 216 Loss:207.708450317 Val loss: 282.278869629\n",
      "Epoch 217 Loss:207.473968506 Val loss: 282.286804199\n",
      "Epoch 218 Loss:207.240753174 Val loss: 282.295684814\n",
      "Epoch 219 Loss:207.008865356 Val loss: 282.305541992\n",
      "Epoch 220 Loss:206.77772522 Val loss: 282.315734863\n",
      "Epoch 221 Loss:206.54725647 Val loss: 282.325958252\n",
      "Epoch 222 Loss:206.31703186 Val loss: 282.33895874\n",
      "Epoch 223 Loss:206.08656311 Val loss: 282.349578857\n",
      "Epoch 224 Loss:205.855834961 Val loss: 282.358978271\n",
      "Epoch 225 Loss:205.626174927 Val loss: 282.366485596\n",
      "Epoch 226 Loss:205.397384644 Val loss: 282.374694824\n",
      "Epoch 227 Loss:205.171829224 Val loss: 282.384765625\n",
      "Epoch 228 Loss:204.947845459 Val loss: 282.393554688\n",
      "Epoch 229 Loss:204.724578857 Val loss: 282.401519775\n",
      "Epoch 230 Loss:204.502349854 Val loss: 282.408081055\n",
      "Epoch 231 Loss:204.281051636 Val loss: 282.413238525\n",
      "Epoch 232 Loss:204.061126709 Val loss: 282.418060303\n",
      "Epoch 233 Loss:203.842498779 Val loss: 282.421875\n",
      "Epoch 234 Loss:203.625076294 Val loss: 282.42489624\n",
      "Epoch 235 Loss:203.408508301 Val loss: 282.426391602\n",
      "Epoch 236 Loss:203.192672729 Val loss: 282.427368164\n",
      "Epoch 237 Loss:202.977478027 Val loss: 282.427856445\n",
      "Epoch 238 Loss:202.762817383 Val loss: 282.428466797\n",
      "Epoch 239 Loss:202.549484253 Val loss: 282.429656982\n",
      "Epoch 240 Loss:202.337249756 Val loss: 282.430175781\n",
      "Epoch 241 Loss:202.12538147 Val loss: 282.430206299\n",
      "Epoch 242 Loss:201.914321899 Val loss: 282.430603027\n",
      "Epoch 243 Loss:201.703842163 Val loss: 282.431640625\n",
      "Epoch 244 Loss:201.49395752 Val loss: 282.431915283\n",
      "Epoch 245 Loss:201.284515381 Val loss: 282.430023193\n",
      "Epoch 246 Loss:201.075180054 Val loss: 282.426757812\n",
      "Epoch 247 Loss:200.866256714 Val loss: 282.423919678\n",
      "Epoch 248 Loss:200.658233643 Val loss: 282.421051025\n",
      "Epoch 249 Loss:200.451019287 Val loss: 282.418457031\n",
      "Epoch 250 Loss:200.244644165 Val loss: 282.416168213\n",
      "Fold 2\n",
      "Epoch 1 Loss:421.819458008 Val loss: 373.094055176\n",
      "Epoch 2 Loss:344.505889893 Val loss: 344.798095703\n",
      "Epoch 3 Loss:336.235870361 Val loss: 341.301940918\n",
      "Epoch 4 Loss:332.641571045 Val loss: 338.847045898\n",
      "Epoch 5 Loss:329.635437012 Val loss: 336.697814941\n",
      "Epoch 6 Loss:326.848022461 Val loss: 334.639953613\n",
      "Epoch 7 Loss:324.124542236 Val loss: 332.573425293\n",
      "Epoch 8 Loss:321.375152588 Val loss: 330.484313965\n",
      "Epoch 9 Loss:318.592132568 Val loss: 328.386993408\n",
      "Epoch 10 Loss:315.838348389 Val loss: 326.3230896\n",
      "Epoch 11 Loss:313.156463623 Val loss: 324.333557129\n",
      "Epoch 12 Loss:310.600891113 Val loss: 322.428070068\n",
      "Epoch 13 Loss:308.195831299 Val loss: 320.616088867\n",
      "Epoch 14 Loss:305.919952393 Val loss: 318.903717041\n",
      "Epoch 15 Loss:303.7605896 Val loss: 317.278594971\n",
      "Epoch 16 Loss:301.69833374 Val loss: 315.735473633\n",
      "Epoch 17 Loss:299.710357666 Val loss: 314.267181396\n",
      "Epoch 18 Loss:297.797027588 Val loss: 312.865783691\n",
      "Epoch 19 Loss:295.960174561 Val loss: 311.515625\n",
      "Epoch 20 Loss:294.206878662 Val loss: 310.232086182\n",
      "Epoch 21 Loss:292.544342041 Val loss: 309.030670166\n",
      "Epoch 22 Loss:290.976287842 Val loss: 307.919647217\n",
      "Epoch 23 Loss:289.504516602 Val loss: 306.884185791\n",
      "Epoch 24 Loss:288.121185303 Val loss: 305.907684326\n",
      "Epoch 25 Loss:286.815246582 Val loss: 304.981262207\n",
      "Epoch 26 Loss:285.579498291 Val loss: 304.099395752\n",
      "Epoch 27 Loss:284.405059814 Val loss: 303.2605896\n",
      "Epoch 28 Loss:283.283172607 Val loss: 302.462127686\n",
      "Epoch 29 Loss:282.206756592 Val loss: 301.702270508\n",
      "Epoch 30 Loss:281.173034668 Val loss: 300.97164917\n",
      "Epoch 31 Loss:280.177825928 Val loss: 300.268463135\n",
      "Epoch 32 Loss:279.219146729 Val loss: 299.588104248\n",
      "Epoch 33 Loss:278.291259766 Val loss: 298.933105469\n",
      "Epoch 34 Loss:277.393127441 Val loss: 298.30960083\n",
      "Epoch 35 Loss:276.529632568 Val loss: 297.719940186\n",
      "Epoch 36 Loss:275.697906494 Val loss: 297.166046143\n",
      "Epoch 37 Loss:274.893554688 Val loss: 296.644622803\n",
      "Epoch 38 Loss:274.116363525 Val loss: 296.153625488\n",
      "Epoch 39 Loss:273.363708496 Val loss: 295.687103271\n",
      "Epoch 40 Loss:272.630859375 Val loss: 295.241485596\n",
      "Epoch 41 Loss:271.912963867 Val loss: 294.814453125\n",
      "Epoch 42 Loss:271.207672119 Val loss: 294.403381348\n",
      "Epoch 43 Loss:270.515136719 Val loss: 294.003326416\n",
      "Epoch 44 Loss:269.836273193 Val loss: 293.616577148\n",
      "Epoch 45 Loss:269.170196533 Val loss: 293.242523193\n",
      "Epoch 46 Loss:268.514892578 Val loss: 292.880950928\n",
      "Epoch 47 Loss:267.870910645 Val loss: 292.533294678\n",
      "Epoch 48 Loss:267.238647461 Val loss: 292.197265625\n",
      "Epoch 49 Loss:266.617797852 Val loss: 291.865661621\n",
      "Epoch 50 Loss:266.008148193 Val loss: 291.549346924\n",
      "Epoch 51 Loss:265.410339355 Val loss: 291.247589111\n",
      "Epoch 52 Loss:264.825164795 Val loss: 290.958496094\n",
      "Epoch 53 Loss:264.250579834 Val loss: 290.675445557\n",
      "Epoch 54 Loss:263.686340332 Val loss: 290.398803711\n",
      "Epoch 55 Loss:263.130950928 Val loss: 290.132141113\n",
      "Epoch 56 Loss:262.582763672 Val loss: 289.875244141\n",
      "Epoch 57 Loss:262.044128418 Val loss: 289.627838135\n",
      "Epoch 58 Loss:261.512664795 Val loss: 289.387023926\n",
      "Epoch 59 Loss:260.987976074 Val loss: 289.150115967\n",
      "Epoch 60 Loss:260.46963501 Val loss: 288.916503906\n",
      "Epoch 61 Loss:259.957366943 Val loss: 288.683959961\n",
      "Epoch 62 Loss:259.450317383 Val loss: 288.453125\n",
      "Epoch 63 Loss:258.946929932 Val loss: 288.231719971\n",
      "Epoch 64 Loss:258.447174072 Val loss: 288.015655518\n",
      "Epoch 65 Loss:257.950958252 Val loss: 287.796783447\n",
      "Epoch 66 Loss:257.458892822 Val loss: 287.573303223\n",
      "Epoch 67 Loss:256.970001221 Val loss: 287.347930908\n",
      "Epoch 68 Loss:256.484527588 Val loss: 287.124847412\n",
      "Epoch 69 Loss:256.002288818 Val loss: 286.905334473\n",
      "Epoch 70 Loss:255.522964478 Val loss: 286.688415527\n",
      "Epoch 71 Loss:255.045272827 Val loss: 286.469299316\n",
      "Epoch 72 Loss:254.569885254 Val loss: 286.250366211\n",
      "Epoch 73 Loss:254.096237183 Val loss: 286.028594971\n",
      "Epoch 74 Loss:253.624282837 Val loss: 285.805114746\n",
      "Epoch 75 Loss:253.155044556 Val loss: 285.581542969\n",
      "Epoch 76 Loss:252.6900177 Val loss: 285.354125977\n",
      "Epoch 77 Loss:252.22958374 Val loss: 285.129150391\n",
      "Epoch 78 Loss:251.773010254 Val loss: 284.907287598\n",
      "Epoch 79 Loss:251.318969727 Val loss: 284.693328857\n",
      "Epoch 80 Loss:250.868774414 Val loss: 284.483917236\n",
      "Epoch 81 Loss:250.421875 Val loss: 284.280792236\n",
      "Epoch 82 Loss:249.977935791 Val loss: 284.082122803\n",
      "Epoch 83 Loss:249.54069519 Val loss: 283.891357422\n",
      "Epoch 84 Loss:249.108032227 Val loss: 283.702789307\n",
      "Epoch 85 Loss:248.681716919 Val loss: 283.519866943\n",
      "Epoch 86 Loss:248.257019043 Val loss: 283.339935303\n",
      "Epoch 87 Loss:247.837188721 Val loss: 283.157958984\n",
      "Epoch 88 Loss:247.415603638 Val loss: 282.978424072\n",
      "Epoch 89 Loss:247.001983643 Val loss: 282.799499512\n",
      "Epoch 90 Loss:246.583496094 Val loss: 282.620605469\n",
      "Epoch 91 Loss:246.176834106 Val loss: 282.440582275\n",
      "Epoch 92 Loss:245.760406494 Val loss: 282.263885498\n",
      "Epoch 93 Loss:245.359909058 Val loss: 282.086883545\n",
      "Epoch 94 Loss:244.951156616 Val loss: 281.918121338\n",
      "Epoch 95 Loss:244.559860229 Val loss: 281.753723145\n",
      "Epoch 96 Loss:244.160079956 Val loss: 281.593444824\n",
      "Epoch 97 Loss:243.774398804 Val loss: 281.432342529\n",
      "Epoch 98 Loss:243.383346558 Val loss: 281.274475098\n",
      "Epoch 99 Loss:243.004882812 Val loss: 281.121704102\n",
      "Epoch 100 Loss:242.624298096 Val loss: 280.969177246\n",
      "Epoch 101 Loss:242.254943848 Val loss: 280.815582275\n",
      "Epoch 102 Loss:241.88117981 Val loss: 280.66217041\n",
      "Epoch 103 Loss:241.517929077 Val loss: 280.504974365\n",
      "Epoch 104 Loss:241.150405884 Val loss: 280.348632812\n",
      "Epoch 105 Loss:240.79246521 Val loss: 280.192016602\n",
      "Epoch 106 Loss:240.429641724 Val loss: 280.041229248\n",
      "Epoch 107 Loss:240.07800293 Val loss: 279.89263916\n",
      "Epoch 108 Loss:239.719345093 Val loss: 279.746856689\n",
      "Epoch 109 Loss:239.3722229 Val loss: 279.601898193\n",
      "Epoch 110 Loss:239.019332886 Val loss: 279.463104248\n",
      "Epoch 111 Loss:238.679855347 Val loss: 279.325012207\n",
      "Epoch 112 Loss:238.334091187 Val loss: 279.19152832\n",
      "Epoch 113 Loss:238.00062561 Val loss: 279.060455322\n",
      "Epoch 114 Loss:237.661300659 Val loss: 278.936767578\n",
      "Epoch 115 Loss:237.333969116 Val loss: 278.815643311\n",
      "Epoch 116 Loss:237.00112915 Val loss: 278.696563721\n",
      "Epoch 117 Loss:236.678375244 Val loss: 278.578826904\n",
      "Epoch 118 Loss:236.350006104 Val loss: 278.464477539\n",
      "Epoch 119 Loss:236.032211304 Val loss: 278.352416992\n",
      "Epoch 120 Loss:235.709640503 Val loss: 278.245849609\n",
      "Epoch 121 Loss:235.398147583 Val loss: 278.139678955\n",
      "Epoch 122 Loss:235.081893921 Val loss: 278.035491943\n",
      "Epoch 123 Loss:234.775741577 Val loss: 277.932098389\n",
      "Epoch 124 Loss:234.464141846 Val loss: 277.831970215\n",
      "Epoch 125 Loss:234.162811279 Val loss: 277.731323242\n",
      "Epoch 126 Loss:233.856048584 Val loss: 277.631652832\n",
      "Epoch 127 Loss:233.558807373 Val loss: 277.530151367\n",
      "Epoch 128 Loss:233.255844116 Val loss: 277.429077148\n",
      "Epoch 129 Loss:232.961532593 Val loss: 277.330291748\n",
      "Epoch 130 Loss:232.661590576 Val loss: 277.235168457\n",
      "Epoch 131 Loss:232.37059021 Val loss: 277.140869141\n",
      "Epoch 132 Loss:232.074050903 Val loss: 277.047668457\n",
      "Epoch 133 Loss:231.786132812 Val loss: 276.953582764\n",
      "Epoch 134 Loss:231.492645264 Val loss: 276.860595703\n",
      "Epoch 135 Loss:231.206863403 Val loss: 276.767822266\n",
      "Epoch 136 Loss:230.91645813 Val loss: 276.673950195\n",
      "Epoch 137 Loss:230.633239746 Val loss: 276.578918457\n",
      "Epoch 138 Loss:230.345672607 Val loss: 276.483276367\n",
      "Epoch 139 Loss:230.064559937 Val loss: 276.387207031\n",
      "Epoch 140 Loss:229.779907227 Val loss: 276.290161133\n",
      "Epoch 141 Loss:229.50100708 Val loss: 276.191619873\n",
      "Epoch 142 Loss:229.21836853 Val loss: 276.092987061\n",
      "Epoch 143 Loss:228.939758301 Val loss: 275.994110107\n",
      "Epoch 144 Loss:228.658218384 Val loss: 275.895507812\n",
      "Epoch 145 Loss:228.381393433 Val loss: 275.797729492\n",
      "Epoch 146 Loss:228.102371216 Val loss: 275.703460693\n",
      "Epoch 147 Loss:227.82598877 Val loss: 275.610107422\n",
      "Epoch 148 Loss:227.546920776 Val loss: 275.516479492\n",
      "Epoch 149 Loss:227.269577026 Val loss: 275.420776367\n",
      "Epoch 150 Loss:226.991271973 Val loss: 275.32510376\n",
      "Epoch 151 Loss:226.714477539 Val loss: 275.227905273\n",
      "Epoch 152 Loss:226.437973022 Val loss: 275.130340576\n",
      "Epoch 153 Loss:226.163192749 Val loss: 275.031646729\n",
      "Epoch 154 Loss:225.889160156 Val loss: 274.93359375\n",
      "Epoch 155 Loss:225.616043091 Val loss: 274.834014893\n",
      "Epoch 156 Loss:225.343780518 Val loss: 274.733398438\n",
      "Epoch 157 Loss:225.073181152 Val loss: 274.633758545\n",
      "Epoch 158 Loss:224.804473877 Val loss: 274.535644531\n",
      "Epoch 159 Loss:224.536621094 Val loss: 274.44052124\n",
      "Epoch 160 Loss:224.269454956 Val loss: 274.345855713\n",
      "Epoch 161 Loss:224.003982544 Val loss: 274.253845215\n",
      "Epoch 162 Loss:223.738449097 Val loss: 274.16192627\n",
      "Epoch 163 Loss:223.474090576 Val loss: 274.07019043\n",
      "Epoch 164 Loss:223.211364746 Val loss: 273.978149414\n",
      "Epoch 165 Loss:222.949768066 Val loss: 273.887878418\n",
      "Epoch 166 Loss:222.688476562 Val loss: 273.799957275\n",
      "Epoch 167 Loss:222.42829895 Val loss: 273.714691162\n",
      "Epoch 168 Loss:222.16847229 Val loss: 273.631561279\n",
      "Epoch 169 Loss:221.90965271 Val loss: 273.553344727\n",
      "Epoch 170 Loss:221.651367188 Val loss: 273.476867676\n",
      "Epoch 171 Loss:221.393829346 Val loss: 273.400512695\n",
      "Epoch 172 Loss:221.13760376 Val loss: 273.324127197\n",
      "Epoch 173 Loss:220.881378174 Val loss: 273.249572754\n",
      "Epoch 174 Loss:220.625717163 Val loss: 273.176086426\n",
      "Epoch 175 Loss:220.370422363 Val loss: 273.104187012\n",
      "Epoch 176 Loss:220.114837646 Val loss: 273.033569336\n",
      "Epoch 177 Loss:219.85925293 Val loss: 272.962738037\n",
      "Epoch 178 Loss:219.603820801 Val loss: 272.892913818\n",
      "Epoch 179 Loss:219.347961426 Val loss: 272.824493408\n",
      "Epoch 180 Loss:219.091690063 Val loss: 272.760437012\n",
      "Epoch 181 Loss:218.835647583 Val loss: 272.696929932\n",
      "Epoch 182 Loss:218.578994751 Val loss: 272.633117676\n",
      "Epoch 183 Loss:218.322280884 Val loss: 272.568145752\n",
      "Epoch 184 Loss:218.064376831 Val loss: 272.498748779\n",
      "Epoch 185 Loss:217.805831909 Val loss: 272.428741455\n",
      "Epoch 186 Loss:217.547424316 Val loss: 272.358520508\n",
      "Epoch 187 Loss:217.288589478 Val loss: 272.288482666\n",
      "Epoch 188 Loss:217.029769897 Val loss: 272.216339111\n",
      "Epoch 189 Loss:216.770996094 Val loss: 272.142669678\n",
      "Epoch 190 Loss:216.511428833 Val loss: 272.070800781\n",
      "Epoch 191 Loss:216.25177002 Val loss: 272.000030518\n",
      "Epoch 192 Loss:215.992492676 Val loss: 271.93081665\n",
      "Epoch 193 Loss:215.732940674 Val loss: 271.863372803\n",
      "Epoch 194 Loss:215.473251343 Val loss: 271.797668457\n",
      "Epoch 195 Loss:215.211807251 Val loss: 271.732452393\n",
      "Epoch 196 Loss:214.950439453 Val loss: 271.66809082\n",
      "Epoch 197 Loss:214.689987183 Val loss: 271.60043335\n",
      "Epoch 198 Loss:214.429779053 Val loss: 271.532562256\n",
      "Epoch 199 Loss:214.169387817 Val loss: 271.464019775\n",
      "Epoch 200 Loss:213.90927124 Val loss: 271.397583008\n",
      "Epoch 201 Loss:213.648773193 Val loss: 271.333740234\n",
      "Epoch 202 Loss:213.3881073 Val loss: 271.272064209\n",
      "Epoch 203 Loss:213.127258301 Val loss: 271.208404541\n",
      "Epoch 204 Loss:212.865753174 Val loss: 271.142608643\n",
      "Epoch 205 Loss:212.604553223 Val loss: 271.074859619\n",
      "Epoch 206 Loss:212.343551636 Val loss: 271.007171631\n",
      "Epoch 207 Loss:212.082107544 Val loss: 270.941864014\n",
      "Epoch 208 Loss:211.819732666 Val loss: 270.876983643\n",
      "Epoch 209 Loss:211.55821228 Val loss: 270.816162109\n",
      "Epoch 210 Loss:211.296920776 Val loss: 270.759246826\n",
      "Epoch 211 Loss:211.03565979 Val loss: 270.702850342\n",
      "Epoch 212 Loss:210.774688721 Val loss: 270.649261475\n",
      "Epoch 213 Loss:210.512695312 Val loss: 270.595672607\n",
      "Epoch 214 Loss:210.250701904 Val loss: 270.54119873\n",
      "Epoch 215 Loss:209.988983154 Val loss: 270.48526001\n",
      "Epoch 216 Loss:209.727783203 Val loss: 270.429504395\n",
      "Epoch 217 Loss:209.467498779 Val loss: 270.373016357\n",
      "Epoch 218 Loss:209.207580566 Val loss: 270.315032959\n",
      "Epoch 219 Loss:208.948242188 Val loss: 270.259429932\n",
      "Epoch 220 Loss:208.689468384 Val loss: 270.204315186\n",
      "Epoch 221 Loss:208.431747437 Val loss: 270.152099609\n",
      "Epoch 222 Loss:208.175521851 Val loss: 270.103179932\n",
      "Epoch 223 Loss:207.919692993 Val loss: 270.053710938\n",
      "Epoch 224 Loss:207.663864136 Val loss: 270.004455566\n",
      "Epoch 225 Loss:207.407516479 Val loss: 269.954711914\n",
      "Epoch 226 Loss:207.150619507 Val loss: 269.903686523\n",
      "Epoch 227 Loss:206.893310547 Val loss: 269.850311279\n",
      "Epoch 228 Loss:206.636505127 Val loss: 269.795684814\n",
      "Epoch 229 Loss:206.37979126 Val loss: 269.740875244\n",
      "Epoch 230 Loss:206.122314453 Val loss: 269.68460083\n",
      "Epoch 231 Loss:205.864547729 Val loss: 269.628570557\n",
      "Epoch 232 Loss:205.6068573 Val loss: 269.574066162\n",
      "Epoch 233 Loss:205.349212646 Val loss: 269.519500732\n",
      "Epoch 234 Loss:205.092025757 Val loss: 269.46697998\n",
      "Epoch 235 Loss:204.836532593 Val loss: 269.412872314\n",
      "Epoch 236 Loss:204.582061768 Val loss: 269.359283447\n",
      "Epoch 237 Loss:204.328552246 Val loss: 269.30770874\n",
      "Epoch 238 Loss:204.075469971 Val loss: 269.256439209\n",
      "Epoch 239 Loss:203.822845459 Val loss: 269.207214355\n",
      "Epoch 240 Loss:203.569061279 Val loss: 269.159179688\n",
      "Epoch 241 Loss:203.314315796 Val loss: 269.115203857\n",
      "Epoch 242 Loss:203.059768677 Val loss: 269.078979492\n",
      "Epoch 243 Loss:202.806228638 Val loss: 269.042633057\n",
      "Epoch 244 Loss:202.553222656 Val loss: 269.007751465\n",
      "Epoch 245 Loss:202.301040649 Val loss: 268.974914551\n",
      "Epoch 246 Loss:202.048217773 Val loss: 268.942993164\n",
      "Epoch 247 Loss:201.796066284 Val loss: 268.91494751\n",
      "Epoch 248 Loss:201.545730591 Val loss: 268.887634277\n",
      "Epoch 249 Loss:201.296508789 Val loss: 268.861022949\n",
      "Epoch 250 Loss:201.048294067 Val loss: 268.840209961\n",
      "Fold 3\n",
      "Epoch 1 Loss:422.906433105 Val loss: 369.203338623\n",
      "Epoch 2 Loss:350.726806641 Val loss: 337.369842529\n",
      "Epoch 3 Loss:337.799163818 Val loss: 332.34576416\n",
      "Epoch 4 Loss:333.898162842 Val loss: 328.905761719\n",
      "Epoch 5 Loss:330.838439941 Val loss: 325.981536865\n",
      "Epoch 6 Loss:328.105194092 Val loss: 323.309692383\n",
      "Epoch 7 Loss:325.485321045 Val loss: 320.789245605\n",
      "Epoch 8 Loss:322.913391113 Val loss: 318.362579346\n",
      "Epoch 9 Loss:320.333007812 Val loss: 315.974151611\n",
      "Epoch 10 Loss:317.742034912 Val loss: 313.643859863\n",
      "Epoch 11 Loss:315.175292969 Val loss: 311.420654297\n",
      "Epoch 12 Loss:312.688171387 Val loss: 309.349945068\n",
      "Epoch 13 Loss:310.323181152 Val loss: 307.42666626\n",
      "Epoch 14 Loss:308.073974609 Val loss: 305.600006104\n",
      "Epoch 15 Loss:305.941009521 Val loss: 303.89352417\n",
      "Epoch 16 Loss:303.910125732 Val loss: 302.263031006\n",
      "Epoch 17 Loss:301.957519531 Val loss: 300.675079346\n",
      "Epoch 18 Loss:300.070800781 Val loss: 299.119750977\n",
      "Epoch 19 Loss:298.237640381 Val loss: 297.59753418\n",
      "Epoch 20 Loss:296.458831787 Val loss: 296.117126465\n",
      "Epoch 21 Loss:294.738372803 Val loss: 294.68157959\n",
      "Epoch 22 Loss:293.088104248 Val loss: 293.308502197\n",
      "Epoch 23 Loss:291.515472412 Val loss: 292.002380371\n",
      "Epoch 24 Loss:290.021820068 Val loss: 290.773925781\n",
      "Epoch 25 Loss:288.609313965 Val loss: 289.630950928\n",
      "Epoch 26 Loss:287.272766113 Val loss: 288.569885254\n",
      "Epoch 27 Loss:286.012359619 Val loss: 287.589080811\n",
      "Epoch 28 Loss:284.823822021 Val loss: 286.681182861\n",
      "Epoch 29 Loss:283.698852539 Val loss: 285.847595215\n",
      "Epoch 30 Loss:282.628295898 Val loss: 285.073394775\n",
      "Epoch 31 Loss:281.601715088 Val loss: 284.348236084\n",
      "Epoch 32 Loss:280.613250732 Val loss: 283.665283203\n",
      "Epoch 33 Loss:279.658081055 Val loss: 283.021575928\n",
      "Epoch 34 Loss:278.729675293 Val loss: 282.411315918\n",
      "Epoch 35 Loss:277.826507568 Val loss: 281.831634521\n",
      "Epoch 36 Loss:276.945037842 Val loss: 281.283691406\n",
      "Epoch 37 Loss:276.085449219 Val loss: 280.760620117\n",
      "Epoch 38 Loss:275.245635986 Val loss: 280.260772705\n",
      "Epoch 39 Loss:274.423950195 Val loss: 279.769836426\n",
      "Epoch 40 Loss:273.613250732 Val loss: 279.287109375\n",
      "Epoch 41 Loss:272.815917969 Val loss: 278.813842773\n",
      "Epoch 42 Loss:272.028656006 Val loss: 278.354858398\n",
      "Epoch 43 Loss:271.254882812 Val loss: 277.913116455\n",
      "Epoch 44 Loss:270.494995117 Val loss: 277.482116699\n",
      "Epoch 45 Loss:269.750335693 Val loss: 277.061706543\n",
      "Epoch 46 Loss:269.019378662 Val loss: 276.647155762\n",
      "Epoch 47 Loss:268.298217773 Val loss: 276.240142822\n",
      "Epoch 48 Loss:267.586730957 Val loss: 275.837341309\n",
      "Epoch 49 Loss:266.885955811 Val loss: 275.444824219\n",
      "Epoch 50 Loss:266.19708252 Val loss: 275.060028076\n",
      "Epoch 51 Loss:265.51864624 Val loss: 274.687896729\n",
      "Epoch 52 Loss:264.84854126 Val loss: 274.328704834\n",
      "Epoch 53 Loss:264.189086914 Val loss: 273.98135376\n",
      "Epoch 54 Loss:263.540557861 Val loss: 273.649047852\n",
      "Epoch 55 Loss:262.901397705 Val loss: 273.328857422\n",
      "Epoch 56 Loss:262.270996094 Val loss: 273.022369385\n",
      "Epoch 57 Loss:261.648620605 Val loss: 272.72442627\n",
      "Epoch 58 Loss:261.034942627 Val loss: 272.434112549\n",
      "Epoch 59 Loss:260.427825928 Val loss: 272.153594971\n",
      "Epoch 60 Loss:259.828521729 Val loss: 271.887420654\n",
      "Epoch 61 Loss:259.236968994 Val loss: 271.631439209\n",
      "Epoch 62 Loss:258.652160645 Val loss: 271.382751465\n",
      "Epoch 63 Loss:258.074066162 Val loss: 271.141418457\n",
      "Epoch 64 Loss:257.502471924 Val loss: 270.904876709\n",
      "Epoch 65 Loss:256.939117432 Val loss: 270.680908203\n",
      "Epoch 66 Loss:256.384552002 Val loss: 270.466339111\n",
      "Epoch 67 Loss:255.837814331 Val loss: 270.253723145\n",
      "Epoch 68 Loss:255.297454834 Val loss: 270.047607422\n",
      "Epoch 69 Loss:254.763519287 Val loss: 269.848571777\n",
      "Epoch 70 Loss:254.236907959 Val loss: 269.656402588\n",
      "Epoch 71 Loss:253.717453003 Val loss: 269.465301514\n",
      "Epoch 72 Loss:253.204711914 Val loss: 269.281921387\n",
      "Epoch 73 Loss:252.697570801 Val loss: 269.102996826\n",
      "Epoch 74 Loss:252.196228027 Val loss: 268.927246094\n",
      "Epoch 75 Loss:251.700485229 Val loss: 268.755767822\n",
      "Epoch 76 Loss:251.208679199 Val loss: 268.58883667\n",
      "Epoch 77 Loss:250.721282959 Val loss: 268.427215576\n",
      "Epoch 78 Loss:250.237915039 Val loss: 268.268798828\n",
      "Epoch 79 Loss:249.75958252 Val loss: 268.110900879\n",
      "Epoch 80 Loss:249.285400391 Val loss: 267.954833984\n",
      "Epoch 81 Loss:248.814086914 Val loss: 267.803283691\n",
      "Epoch 82 Loss:248.346282959 Val loss: 267.66027832\n",
      "Epoch 83 Loss:247.881500244 Val loss: 267.521972656\n",
      "Epoch 84 Loss:247.420440674 Val loss: 267.385894775\n",
      "Epoch 85 Loss:246.96295166 Val loss: 267.25289917\n",
      "Epoch 86 Loss:246.508529663 Val loss: 267.127807617\n",
      "Epoch 87 Loss:246.059188843 Val loss: 267.00692749\n",
      "Epoch 88 Loss:245.614212036 Val loss: 266.890441895\n",
      "Epoch 89 Loss:245.173355103 Val loss: 266.779266357\n",
      "Epoch 90 Loss:244.735809326 Val loss: 266.670898438\n",
      "Epoch 91 Loss:244.300521851 Val loss: 266.567504883\n",
      "Epoch 92 Loss:243.868270874 Val loss: 266.461029053\n",
      "Epoch 93 Loss:243.437805176 Val loss: 266.359069824\n",
      "Epoch 94 Loss:243.010955811 Val loss: 266.255523682\n",
      "Epoch 95 Loss:242.586853027 Val loss: 266.154418945\n",
      "Epoch 96 Loss:242.165878296 Val loss: 266.052185059\n",
      "Epoch 97 Loss:241.747970581 Val loss: 265.950927734\n",
      "Epoch 98 Loss:241.333633423 Val loss: 265.843475342\n",
      "Epoch 99 Loss:240.921279907 Val loss: 265.739440918\n",
      "Epoch 100 Loss:240.511077881 Val loss: 265.634521484\n",
      "Epoch 101 Loss:240.102355957 Val loss: 265.537506104\n",
      "Epoch 102 Loss:239.697616577 Val loss: 265.440948486\n",
      "Epoch 103 Loss:239.29699707 Val loss: 265.348999023\n",
      "Epoch 104 Loss:238.901031494 Val loss: 265.259002686\n",
      "Epoch 105 Loss:238.509185791 Val loss: 265.171600342\n",
      "Epoch 106 Loss:238.120834351 Val loss: 265.087982178\n",
      "Epoch 107 Loss:237.736465454 Val loss: 265.010131836\n",
      "Epoch 108 Loss:237.355514526 Val loss: 264.93637085\n",
      "Epoch 109 Loss:236.978713989 Val loss: 264.865692139\n",
      "Epoch 110 Loss:236.606719971 Val loss: 264.797149658\n",
      "Epoch 111 Loss:236.239120483 Val loss: 264.731445312\n",
      "Epoch 112 Loss:235.875244141 Val loss: 264.672271729\n",
      "Epoch 113 Loss:235.515045166 Val loss: 264.613830566\n",
      "Epoch 114 Loss:235.157836914 Val loss: 264.561096191\n",
      "Epoch 115 Loss:234.804092407 Val loss: 264.507019043\n",
      "Epoch 116 Loss:234.453796387 Val loss: 264.459472656\n",
      "Epoch 117 Loss:234.107269287 Val loss: 264.410095215\n",
      "Epoch 118 Loss:233.763870239 Val loss: 264.371734619\n",
      "Epoch 119 Loss:233.424407959 Val loss: 264.324707031\n",
      "Epoch 120 Loss:233.086990356 Val loss: 264.295471191\n",
      "Epoch 121 Loss:232.753433228 Val loss: 264.244781494\n",
      "Epoch 122 Loss:232.41973877 Val loss: 264.22845459\n",
      "Epoch 123 Loss:232.092697144 Val loss: 264.161834717\n",
      "Epoch 124 Loss:231.763153076 Val loss: 264.168060303\n",
      "Epoch 125 Loss:231.442718506 Val loss: 264.055938721\n",
      "Epoch 126 Loss:231.111572266 Val loss: 264.127258301\n",
      "Epoch 127 Loss:230.800979614 Val loss: 263.91897583\n",
      "Epoch 128 Loss:230.466766357 Val loss: 264.113128662\n",
      "Epoch 129 Loss:230.170333862 Val loss: 263.786712646\n",
      "Epoch 130 Loss:229.829833984 Val loss: 264.060424805\n",
      "Epoch 131 Loss:229.536712646 Val loss: 263.733520508\n",
      "Epoch 132 Loss:229.199264526 Val loss: 263.960845947\n",
      "Epoch 133 Loss:228.90411377 Val loss: 263.706848145\n",
      "Epoch 134 Loss:228.575393677 Val loss: 263.857940674\n",
      "Epoch 135 Loss:228.278717041 Val loss: 263.666778564\n",
      "Epoch 136 Loss:227.95703125 Val loss: 263.770446777\n",
      "Epoch 137 Loss:227.658981323 Val loss: 263.612182617\n",
      "Epoch 138 Loss:227.3409729 Val loss: 263.694244385\n",
      "Epoch 139 Loss:227.044281006 Val loss: 263.547576904\n",
      "Epoch 140 Loss:226.728759766 Val loss: 263.621612549\n",
      "Epoch 141 Loss:226.433761597 Val loss: 263.471984863\n",
      "Epoch 142 Loss:226.119293213 Val loss: 263.558868408\n",
      "Epoch 143 Loss:225.827713013 Val loss: 263.406463623\n",
      "Epoch 144 Loss:225.51600647 Val loss: 263.503997803\n",
      "Epoch 145 Loss:225.228866577 Val loss: 263.346679688\n",
      "Epoch 146 Loss:224.917877197 Val loss: 263.45703125\n",
      "Epoch 147 Loss:224.634078979 Val loss: 263.292541504\n",
      "Epoch 148 Loss:224.323760986 Val loss: 263.410430908\n",
      "Epoch 149 Loss:224.042373657 Val loss: 263.245269775\n",
      "Epoch 150 Loss:223.732177734 Val loss: 263.368652344\n",
      "Epoch 151 Loss:223.453216553 Val loss: 263.207366943\n",
      "Epoch 152 Loss:223.144821167 Val loss: 263.333282471\n",
      "Epoch 153 Loss:222.868148804 Val loss: 263.172851562\n",
      "Epoch 154 Loss:222.560791016 Val loss: 263.300994873\n",
      "Epoch 155 Loss:222.285949707 Val loss: 263.140899658\n",
      "Epoch 156 Loss:221.979492188 Val loss: 263.277099609\n",
      "Epoch 157 Loss:221.707458496 Val loss: 263.1121521\n",
      "Epoch 158 Loss:221.400787354 Val loss: 263.258361816\n",
      "Epoch 159 Loss:221.13041687 Val loss: 263.08190918\n",
      "Epoch 160 Loss:220.822036743 Val loss: 263.246826172\n",
      "Epoch 161 Loss:220.553970337 Val loss: 263.057800293\n",
      "Epoch 162 Loss:220.243148804 Val loss: 263.24621582\n",
      "Epoch 163 Loss:219.978622437 Val loss: 263.035430908\n",
      "Epoch 164 Loss:219.665618896 Val loss: 263.24041748\n",
      "Epoch 165 Loss:219.4037323 Val loss: 263.009979248\n",
      "Epoch 166 Loss:219.087997437 Val loss: 263.223175049\n",
      "Epoch 167 Loss:218.827377319 Val loss: 263.000701904\n",
      "Epoch 168 Loss:218.511047363 Val loss: 263.207824707\n",
      "Epoch 169 Loss:218.250549316 Val loss: 263.006591797\n",
      "Epoch 170 Loss:217.935821533 Val loss: 263.210723877\n",
      "Epoch 171 Loss:217.677810669 Val loss: 263.026428223\n",
      "Epoch 172 Loss:217.366271973 Val loss: 263.220733643\n",
      "Epoch 173 Loss:217.111465454 Val loss: 263.046081543\n",
      "Epoch 174 Loss:216.801208496 Val loss: 263.238677979\n",
      "Epoch 175 Loss:216.548934937 Val loss: 263.068817139\n",
      "Epoch 176 Loss:216.238449097 Val loss: 263.267913818\n",
      "Epoch 177 Loss:215.990921021 Val loss: 263.093566895\n",
      "Epoch 178 Loss:215.680831909 Val loss: 263.29876709\n",
      "Epoch 179 Loss:215.438308716 Val loss: 263.107025146\n",
      "Epoch 180 Loss:215.125579834 Val loss: 263.333282471\n",
      "Epoch 181 Loss:214.88822937 Val loss: 263.132751465\n",
      "Epoch 182 Loss:214.575515747 Val loss: 263.360656738\n",
      "Epoch 183 Loss:214.339431763 Val loss: 263.166534424\n",
      "Epoch 184 Loss:214.028610229 Val loss: 263.381561279\n",
      "Epoch 185 Loss:213.79145813 Val loss: 263.198944092\n",
      "Epoch 186 Loss:213.484573364 Val loss: 263.399902344\n",
      "Epoch 187 Loss:213.247573853 Val loss: 263.228088379\n",
      "Epoch 188 Loss:212.944107056 Val loss: 263.42779541\n",
      "Epoch 189 Loss:212.709014893 Val loss: 263.256988525\n",
      "Epoch 190 Loss:212.406448364 Val loss: 263.467437744\n",
      "Epoch 191 Loss:212.175018311 Val loss: 263.289794922\n",
      "Epoch 192 Loss:211.871795654 Val loss: 263.522033691\n",
      "Epoch 193 Loss:211.645507812 Val loss: 263.338806152\n",
      "Epoch 194 Loss:211.340560913 Val loss: 263.589111328\n",
      "Epoch 195 Loss:211.117385864 Val loss: 263.397644043\n",
      "Epoch 196 Loss:210.811965942 Val loss: 263.651580811\n",
      "Epoch 197 Loss:210.591827393 Val loss: 263.46697998\n",
      "Epoch 198 Loss:210.287750244 Val loss: 263.717285156\n",
      "Epoch 199 Loss:210.071685791 Val loss: 263.535888672\n",
      "Epoch 200 Loss:209.770004272 Val loss: 263.773651123\n",
      "Epoch 201 Loss:209.556289673 Val loss: 263.592803955\n",
      "Epoch 202 Loss:209.255874634 Val loss: 263.825469971\n",
      "Epoch 203 Loss:209.045059204 Val loss: 263.642211914\n",
      "Epoch 204 Loss:208.742996216 Val loss: 263.880737305\n",
      "Epoch 205 Loss:208.536712646 Val loss: 263.684204102\n",
      "Epoch 206 Loss:208.233657837 Val loss: 263.930938721\n",
      "Epoch 207 Loss:208.031494141 Val loss: 263.720062256\n",
      "Epoch 208 Loss:207.726760864 Val loss: 263.972747803\n",
      "Epoch 209 Loss:207.527038574 Val loss: 263.747039795\n",
      "Epoch 210 Loss:207.22052002 Val loss: 263.996276855\n",
      "Epoch 211 Loss:207.020507812 Val loss: 263.764251709\n",
      "Epoch 212 Loss:206.716522217 Val loss: 264.01071167\n",
      "Epoch 213 Loss:206.518692017 Val loss: 263.776062012\n",
      "Epoch 214 Loss:206.218414307 Val loss: 264.026611328\n",
      "Epoch 215 Loss:206.021850586 Val loss: 263.788391113\n",
      "Epoch 216 Loss:205.724304199 Val loss: 264.058776855\n",
      "Epoch 217 Loss:205.531326294 Val loss: 263.800445557\n",
      "Epoch 218 Loss:205.23348999 Val loss: 264.100402832\n",
      "Epoch 219 Loss:205.042053223 Val loss: 263.823394775\n",
      "Epoch 220 Loss:204.744064331 Val loss: 264.148162842\n",
      "Epoch 221 Loss:204.553970337 Val loss: 263.859100342\n",
      "Epoch 222 Loss:204.253479004 Val loss: 264.174713135\n",
      "Epoch 223 Loss:204.058685303 Val loss: 263.8956604\n",
      "Epoch 224 Loss:203.761413574 Val loss: 264.192779541\n",
      "Epoch 225 Loss:203.559814453 Val loss: 263.933624268\n",
      "Epoch 226 Loss:203.26739502 Val loss: 264.218444824\n",
      "Epoch 227 Loss:203.061523438 Val loss: 263.983673096\n",
      "Epoch 228 Loss:202.775115967 Val loss: 264.260650635\n",
      "Epoch 229 Loss:202.567321777 Val loss: 264.036315918\n",
      "Epoch 230 Loss:202.286331177 Val loss: 264.316589355\n",
      "Epoch 231 Loss:202.081390381 Val loss: 264.08770752\n",
      "Epoch 232 Loss:201.803817749 Val loss: 264.385009766\n",
      "Epoch 233 Loss:201.603271484 Val loss: 264.137695312\n",
      "Epoch 234 Loss:201.326446533 Val loss: 264.45690918\n",
      "Epoch 235 Loss:201.128814697 Val loss: 264.20300293\n",
      "Epoch 236 Loss:200.849609375 Val loss: 264.521575928\n",
      "Epoch 237 Loss:200.650863647 Val loss: 264.287719727\n",
      "Epoch 238 Loss:200.373184204 Val loss: 264.583129883\n",
      "Epoch 239 Loss:200.173873901 Val loss: 264.37802124\n",
      "Epoch 240 Loss:199.899536133 Val loss: 264.655181885\n",
      "Epoch 241 Loss:199.70401001 Val loss: 264.46295166\n",
      "Epoch 242 Loss:199.431777954 Val loss: 264.732391357\n",
      "Epoch 243 Loss:199.241088867 Val loss: 264.53225708\n",
      "Epoch 244 Loss:198.966293335 Val loss: 264.809783936\n",
      "Epoch 245 Loss:198.781204224 Val loss: 264.591156006\n",
      "Epoch 246 Loss:198.502243042 Val loss: 264.887695312\n",
      "Epoch 247 Loss:198.322128296 Val loss: 264.653259277\n",
      "Epoch 248 Loss:198.038467407 Val loss: 264.968414307\n",
      "Epoch 249 Loss:197.861312866 Val loss: 264.724151611\n",
      "Epoch 250 Loss:197.574768066 Val loss: 265.042236328\n",
      "Fold 4\n",
      "Epoch 1 Loss:411.23638916 Val loss: 433.502838135\n",
      "Epoch 2 Loss:334.617706299 Val loss: 406.409240723\n",
      "Epoch 3 Loss:326.263763428 Val loss: 401.749481201\n",
      "Epoch 4 Loss:322.716064453 Val loss: 398.47265625\n",
      "Epoch 5 Loss:319.848693848 Val loss: 395.681152344\n",
      "Epoch 6 Loss:317.126586914 Val loss: 393.028869629\n",
      "Epoch 7 Loss:314.338623047 Val loss: 390.319396973\n",
      "Epoch 8 Loss:311.415344238 Val loss: 387.545928955\n",
      "Epoch 9 Loss:308.392028809 Val loss: 384.793060303\n",
      "Epoch 10 Loss:305.379394531 Val loss: 382.126098633\n",
      "Epoch 11 Loss:302.484893799 Val loss: 379.600830078\n",
      "Epoch 12 Loss:299.766784668 Val loss: 377.220184326\n",
      "Epoch 13 Loss:297.227416992 Val loss: 374.989471436\n",
      "Epoch 14 Loss:294.854888916 Val loss: 372.918914795\n",
      "Epoch 15 Loss:292.661895752 Val loss: 371.045288086\n",
      "Epoch 16 Loss:290.642578125 Val loss: 369.330718994\n",
      "Epoch 17 Loss:288.779632568 Val loss: 367.752227783\n",
      "Epoch 18 Loss:287.050720215 Val loss: 366.295074463\n",
      "Epoch 19 Loss:285.447662354 Val loss: 364.946563721\n",
      "Epoch 20 Loss:283.955749512 Val loss: 363.693145752\n",
      "Epoch 21 Loss:282.563354492 Val loss: 362.528564453\n",
      "Epoch 22 Loss:281.254852295 Val loss: 361.426574707\n",
      "Epoch 23 Loss:280.014801025 Val loss: 360.379852295\n",
      "Epoch 24 Loss:278.834197998 Val loss: 359.374938965\n",
      "Epoch 25 Loss:277.704467773 Val loss: 358.400421143\n",
      "Epoch 26 Loss:276.612121582 Val loss: 357.457000732\n",
      "Epoch 27 Loss:275.552642822 Val loss: 356.541809082\n",
      "Epoch 28 Loss:274.522186279 Val loss: 355.648406982\n",
      "Epoch 29 Loss:273.523376465 Val loss: 354.78326416\n",
      "Epoch 30 Loss:272.552856445 Val loss: 353.9453125\n",
      "Epoch 31 Loss:271.605529785 Val loss: 353.113708496\n",
      "Epoch 32 Loss:270.67767334 Val loss: 352.300720215\n",
      "Epoch 33 Loss:269.769470215 Val loss: 351.499908447\n",
      "Epoch 34 Loss:268.877410889 Val loss: 350.714447021\n",
      "Epoch 35 Loss:267.996246338 Val loss: 349.938415527\n",
      "Epoch 36 Loss:267.127441406 Val loss: 349.18347168\n",
      "Epoch 37 Loss:266.271728516 Val loss: 348.440734863\n",
      "Epoch 38 Loss:265.428741455 Val loss: 347.721801758\n",
      "Epoch 39 Loss:264.59564209 Val loss: 347.019348145\n",
      "Epoch 40 Loss:263.773925781 Val loss: 346.327484131\n",
      "Epoch 41 Loss:262.962219238 Val loss: 345.651367188\n",
      "Epoch 42 Loss:262.162658691 Val loss: 344.996551514\n",
      "Epoch 43 Loss:261.370758057 Val loss: 344.347717285\n",
      "Epoch 44 Loss:260.587768555 Val loss: 343.705078125\n",
      "Epoch 45 Loss:259.812774658 Val loss: 343.071411133\n",
      "Epoch 46 Loss:259.043914795 Val loss: 342.449432373\n",
      "Epoch 47 Loss:258.284088135 Val loss: 341.843353271\n",
      "Epoch 48 Loss:257.534454346 Val loss: 341.246582031\n",
      "Epoch 49 Loss:256.792236328 Val loss: 340.659057617\n",
      "Epoch 50 Loss:256.060455322 Val loss: 340.075836182\n",
      "Epoch 51 Loss:255.343551636 Val loss: 339.490203857\n",
      "Epoch 52 Loss:254.638656616 Val loss: 338.918518066\n",
      "Epoch 53 Loss:253.943389893 Val loss: 338.352172852\n",
      "Epoch 54 Loss:253.261169434 Val loss: 337.78994751\n",
      "Epoch 55 Loss:252.586746216 Val loss: 337.233642578\n",
      "Epoch 56 Loss:251.920654297 Val loss: 336.671569824\n",
      "Epoch 57 Loss:251.262084961 Val loss: 336.109649658\n",
      "Epoch 58 Loss:250.61517334 Val loss: 335.553527832\n",
      "Epoch 59 Loss:249.978820801 Val loss: 335.00189209\n",
      "Epoch 60 Loss:249.351425171 Val loss: 334.455078125\n",
      "Epoch 61 Loss:248.731674194 Val loss: 333.918914795\n",
      "Epoch 62 Loss:248.122360229 Val loss: 333.392242432\n",
      "Epoch 63 Loss:247.520172119 Val loss: 332.867553711\n",
      "Epoch 64 Loss:246.924072266 Val loss: 332.357330322\n",
      "Epoch 65 Loss:246.334762573 Val loss: 331.862304688\n",
      "Epoch 66 Loss:245.750335693 Val loss: 331.376190186\n",
      "Epoch 67 Loss:245.171539307 Val loss: 330.89251709\n",
      "Epoch 68 Loss:244.596725464 Val loss: 330.421264648\n",
      "Epoch 69 Loss:244.027084351 Val loss: 329.950897217\n",
      "Epoch 70 Loss:243.464385986 Val loss: 329.480834961\n",
      "Epoch 71 Loss:242.908309937 Val loss: 329.01574707\n",
      "Epoch 72 Loss:242.361633301 Val loss: 328.549255371\n",
      "Epoch 73 Loss:241.82194519 Val loss: 328.091461182\n",
      "Epoch 74 Loss:241.28717041 Val loss: 327.640533447\n",
      "Epoch 75 Loss:240.75579834 Val loss: 327.19732666\n",
      "Epoch 76 Loss:240.226425171 Val loss: 326.764801025\n",
      "Epoch 77 Loss:239.702453613 Val loss: 326.336120605\n",
      "Epoch 78 Loss:239.1847229 Val loss: 325.913024902\n",
      "Epoch 79 Loss:238.670394897 Val loss: 325.492645264\n",
      "Epoch 80 Loss:238.160903931 Val loss: 325.072937012\n",
      "Epoch 81 Loss:237.655822754 Val loss: 324.656280518\n",
      "Epoch 82 Loss:237.154907227 Val loss: 324.239624023\n",
      "Epoch 83 Loss:236.65637207 Val loss: 323.825469971\n",
      "Epoch 84 Loss:236.16178894 Val loss: 323.416503906\n",
      "Epoch 85 Loss:235.670532227 Val loss: 323.013580322\n",
      "Epoch 86 Loss:235.182327271 Val loss: 322.614685059\n",
      "Epoch 87 Loss:234.697799683 Val loss: 322.218841553\n",
      "Epoch 88 Loss:234.215774536 Val loss: 321.828308105\n",
      "Epoch 89 Loss:233.73638916 Val loss: 321.442138672\n",
      "Epoch 90 Loss:233.25958252 Val loss: 321.052337646\n",
      "Epoch 91 Loss:232.78465271 Val loss: 320.668182373\n",
      "Epoch 92 Loss:232.312362671 Val loss: 320.290527344\n",
      "Epoch 93 Loss:231.842071533 Val loss: 319.91973877\n",
      "Epoch 94 Loss:231.375762939 Val loss: 319.556793213\n",
      "Epoch 95 Loss:230.911193848 Val loss: 319.199768066\n",
      "Epoch 96 Loss:230.450363159 Val loss: 318.843200684\n",
      "Epoch 97 Loss:229.99256897 Val loss: 318.485748291\n",
      "Epoch 98 Loss:229.538146973 Val loss: 318.134124756\n",
      "Epoch 99 Loss:229.08807373 Val loss: 317.789001465\n",
      "Epoch 100 Loss:228.642440796 Val loss: 317.454742432\n",
      "Epoch 101 Loss:228.20173645 Val loss: 317.126403809\n",
      "Epoch 102 Loss:227.766387939 Val loss: 316.80078125\n",
      "Epoch 103 Loss:227.334762573 Val loss: 316.479522705\n",
      "Epoch 104 Loss:226.905822754 Val loss: 316.161193848\n",
      "Epoch 105 Loss:226.479873657 Val loss: 315.846618652\n",
      "Epoch 106 Loss:226.05632019 Val loss: 315.535400391\n",
      "Epoch 107 Loss:225.635345459 Val loss: 315.233978271\n",
      "Epoch 108 Loss:225.217437744 Val loss: 314.941955566\n",
      "Epoch 109 Loss:224.802429199 Val loss: 314.654968262\n",
      "Epoch 110 Loss:224.38973999 Val loss: 314.3699646\n",
      "Epoch 111 Loss:223.978485107 Val loss: 314.089202881\n",
      "Epoch 112 Loss:223.571273804 Val loss: 313.811798096\n",
      "Epoch 113 Loss:223.169723511 Val loss: 313.53616333\n",
      "Epoch 114 Loss:222.771133423 Val loss: 313.265991211\n",
      "Epoch 115 Loss:222.377349854 Val loss: 312.997589111\n",
      "Epoch 116 Loss:221.988449097 Val loss: 312.732391357\n",
      "Epoch 117 Loss:221.604003906 Val loss: 312.471282959\n",
      "Epoch 118 Loss:221.22328186 Val loss: 312.215179443\n",
      "Epoch 119 Loss:220.845565796 Val loss: 311.968048096\n",
      "Epoch 120 Loss:220.472061157 Val loss: 311.730987549\n",
      "Epoch 121 Loss:220.102996826 Val loss: 311.500793457\n",
      "Epoch 122 Loss:219.737747192 Val loss: 311.274108887\n",
      "Epoch 123 Loss:219.375091553 Val loss: 311.04788208\n",
      "Epoch 124 Loss:219.01361084 Val loss: 310.8203125\n",
      "Epoch 125 Loss:218.654785156 Val loss: 310.594146729\n",
      "Epoch 126 Loss:218.299606323 Val loss: 310.371398926\n",
      "Epoch 127 Loss:217.947296143 Val loss: 310.153198242\n",
      "Epoch 128 Loss:217.598587036 Val loss: 309.941467285\n",
      "Epoch 129 Loss:217.253036499 Val loss: 309.732666016\n",
      "Epoch 130 Loss:216.910903931 Val loss: 309.527496338\n",
      "Epoch 131 Loss:216.572036743 Val loss: 309.324554443\n",
      "Epoch 132 Loss:216.23576355 Val loss: 309.122955322\n",
      "Epoch 133 Loss:215.902770996 Val loss: 308.923980713\n",
      "Epoch 134 Loss:215.573074341 Val loss: 308.728302002\n",
      "Epoch 135 Loss:215.24597168 Val loss: 308.536224365\n",
      "Epoch 136 Loss:214.921157837 Val loss: 308.345855713\n",
      "Epoch 137 Loss:214.598587036 Val loss: 308.157775879\n",
      "Epoch 138 Loss:214.278488159 Val loss: 307.972045898\n",
      "Epoch 139 Loss:213.961227417 Val loss: 307.786865234\n",
      "Epoch 140 Loss:213.646179199 Val loss: 307.603729248\n",
      "Epoch 141 Loss:213.333679199 Val loss: 307.423156738\n",
      "Epoch 142 Loss:213.023132324 Val loss: 307.245727539\n",
      "Epoch 143 Loss:212.714599609 Val loss: 307.070617676\n",
      "Epoch 144 Loss:212.407974243 Val loss: 306.89666748\n",
      "Epoch 145 Loss:212.10319519 Val loss: 306.723815918\n",
      "Epoch 146 Loss:211.7996521 Val loss: 306.553100586\n",
      "Epoch 147 Loss:211.497009277 Val loss: 306.384277344\n",
      "Epoch 148 Loss:211.196075439 Val loss: 306.218444824\n",
      "Epoch 149 Loss:210.897521973 Val loss: 306.054260254\n",
      "Epoch 150 Loss:210.600021362 Val loss: 305.892303467\n",
      "Epoch 151 Loss:210.305221558 Val loss: 305.733306885\n",
      "Epoch 152 Loss:210.012512207 Val loss: 305.577606201\n",
      "Epoch 153 Loss:209.721786499 Val loss: 305.425842285\n",
      "Epoch 154 Loss:209.432342529 Val loss: 305.275787354\n",
      "Epoch 155 Loss:209.143783569 Val loss: 305.12802124\n",
      "Epoch 156 Loss:208.857223511 Val loss: 304.981781006\n",
      "Epoch 157 Loss:208.571853638 Val loss: 304.83972168\n",
      "Epoch 158 Loss:208.287963867 Val loss: 304.700408936\n",
      "Epoch 159 Loss:208.004074097 Val loss: 304.564910889\n",
      "Epoch 160 Loss:207.721450806 Val loss: 304.432861328\n",
      "Epoch 161 Loss:207.440185547 Val loss: 304.302734375\n",
      "Epoch 162 Loss:207.16065979 Val loss: 304.174835205\n",
      "Epoch 163 Loss:206.883102417 Val loss: 304.049804688\n",
      "Epoch 164 Loss:206.606307983 Val loss: 303.927398682\n",
      "Epoch 165 Loss:206.329498291 Val loss: 303.807250977\n",
      "Epoch 166 Loss:206.053634644 Val loss: 303.689453125\n",
      "Epoch 167 Loss:205.778610229 Val loss: 303.576202393\n",
      "Epoch 168 Loss:205.505325317 Val loss: 303.46383667\n",
      "Epoch 169 Loss:205.234039307 Val loss: 303.35043335\n",
      "Epoch 170 Loss:204.964126587 Val loss: 303.239593506\n",
      "Epoch 171 Loss:204.696029663 Val loss: 303.131256104\n",
      "Epoch 172 Loss:204.429107666 Val loss: 303.02532959\n",
      "Epoch 173 Loss:204.163192749 Val loss: 302.921142578\n",
      "Epoch 174 Loss:203.898391724 Val loss: 302.818817139\n",
      "Epoch 175 Loss:203.635299683 Val loss: 302.720306396\n",
      "Epoch 176 Loss:203.37298584 Val loss: 302.623687744\n",
      "Epoch 177 Loss:203.112701416 Val loss: 302.530273438\n",
      "Epoch 178 Loss:202.854202271 Val loss: 302.440032959\n",
      "Epoch 179 Loss:202.597747803 Val loss: 302.351654053\n",
      "Epoch 180 Loss:202.342498779 Val loss: 302.264801025\n",
      "Epoch 181 Loss:202.08833313 Val loss: 302.179351807\n",
      "Epoch 182 Loss:201.834594727 Val loss: 302.097198486\n",
      "Epoch 183 Loss:201.581329346 Val loss: 302.017578125\n",
      "Epoch 184 Loss:201.328933716 Val loss: 301.937957764\n",
      "Epoch 185 Loss:201.077255249 Val loss: 301.858520508\n",
      "Epoch 186 Loss:200.82673645 Val loss: 301.77835083\n",
      "Epoch 187 Loss:200.577255249 Val loss: 301.697479248\n",
      "Epoch 188 Loss:200.328826904 Val loss: 301.617767334\n",
      "Epoch 189 Loss:200.080413818 Val loss: 301.539031982\n",
      "Epoch 190 Loss:199.83253479 Val loss: 301.460021973\n",
      "Epoch 191 Loss:199.58555603 Val loss: 301.37902832\n",
      "Epoch 192 Loss:199.340896606 Val loss: 301.298950195\n",
      "Epoch 193 Loss:199.097702026 Val loss: 301.223327637\n",
      "Epoch 194 Loss:198.852859497 Val loss: 301.148468018\n",
      "Epoch 195 Loss:198.609008789 Val loss: 301.074798584\n",
      "Epoch 196 Loss:198.366912842 Val loss: 301.002258301\n",
      "Epoch 197 Loss:198.125289917 Val loss: 300.931488037\n",
      "Epoch 198 Loss:197.883728027 Val loss: 300.861816406\n",
      "Epoch 199 Loss:197.642440796 Val loss: 300.793457031\n",
      "Epoch 200 Loss:197.402130127 Val loss: 300.725402832\n",
      "Epoch 201 Loss:197.163085938 Val loss: 300.657165527\n",
      "Epoch 202 Loss:196.923171997 Val loss: 300.589141846\n",
      "Epoch 203 Loss:196.68409729 Val loss: 300.521575928\n",
      "Epoch 204 Loss:196.447128296 Val loss: 300.453063965\n",
      "Epoch 205 Loss:196.210403442 Val loss: 300.38458252\n",
      "Epoch 206 Loss:195.974639893 Val loss: 300.317962646\n",
      "Epoch 207 Loss:195.739089966 Val loss: 300.251861572\n",
      "Epoch 208 Loss:195.504989624 Val loss: 300.186767578\n",
      "Epoch 209 Loss:195.271972656 Val loss: 300.123291016\n",
      "Epoch 210 Loss:195.03918457 Val loss: 300.061157227\n",
      "Epoch 211 Loss:194.806442261 Val loss: 299.999298096\n",
      "Epoch 212 Loss:194.574127197 Val loss: 299.938415527\n",
      "Epoch 213 Loss:194.342010498 Val loss: 299.879730225\n",
      "Epoch 214 Loss:194.109802246 Val loss: 299.821350098\n",
      "Epoch 215 Loss:193.876541138 Val loss: 299.76171875\n",
      "Epoch 216 Loss:193.643310547 Val loss: 299.702453613\n",
      "Epoch 217 Loss:193.410293579 Val loss: 299.642303467\n",
      "Epoch 218 Loss:193.177246094 Val loss: 299.582366943\n",
      "Epoch 219 Loss:192.944274902 Val loss: 299.519897461\n",
      "Epoch 220 Loss:192.711639404 Val loss: 299.455963135\n",
      "Epoch 221 Loss:192.479858398 Val loss: 299.391448975\n",
      "Epoch 222 Loss:192.248657227 Val loss: 299.325927734\n",
      "Epoch 223 Loss:192.017364502 Val loss: 299.259613037\n",
      "Epoch 224 Loss:191.784835815 Val loss: 299.18927002\n",
      "Epoch 225 Loss:191.550231934 Val loss: 299.123443604\n",
      "Epoch 226 Loss:191.314041138 Val loss: 299.06350708\n",
      "Epoch 227 Loss:191.077926636 Val loss: 299.000915527\n",
      "Epoch 228 Loss:190.843261719 Val loss: 298.936828613\n",
      "Epoch 229 Loss:190.609237671 Val loss: 298.864898682\n",
      "Epoch 230 Loss:190.374801636 Val loss: 298.792266846\n",
      "Epoch 231 Loss:190.141265869 Val loss: 298.720825195\n",
      "Epoch 232 Loss:189.908843994 Val loss: 298.648010254\n",
      "Epoch 233 Loss:189.677886963 Val loss: 298.576202393\n",
      "Epoch 234 Loss:189.4480896 Val loss: 298.501678467\n",
      "Epoch 235 Loss:189.219055176 Val loss: 298.428375244\n",
      "Epoch 236 Loss:188.99118042 Val loss: 298.352172852\n",
      "Epoch 237 Loss:188.763656616 Val loss: 298.278411865\n",
      "Epoch 238 Loss:188.536468506 Val loss: 298.206665039\n",
      "Epoch 239 Loss:188.309951782 Val loss: 298.138397217\n",
      "Epoch 240 Loss:188.084014893 Val loss: 298.070800781\n",
      "Epoch 241 Loss:187.858459473 Val loss: 298.005432129\n",
      "Epoch 242 Loss:187.633453369 Val loss: 297.941101074\n",
      "Epoch 243 Loss:187.408950806 Val loss: 297.878540039\n",
      "Epoch 244 Loss:187.18598938 Val loss: 297.815155029\n",
      "Epoch 245 Loss:186.964035034 Val loss: 297.756195068\n",
      "Epoch 246 Loss:186.743728638 Val loss: 297.695007324\n",
      "Epoch 247 Loss:186.524261475 Val loss: 297.641265869\n",
      "Epoch 248 Loss:186.305999756 Val loss: 297.586303711\n",
      "Epoch 249 Loss:186.088699341 Val loss: 297.541442871\n",
      "Epoch 250 Loss:185.873153687 Val loss: 297.49029541\n",
      "Fold 5\n",
      "Epoch 1 Loss:416.055450439 Val loss: 362.477386475\n",
      "Epoch 2 Loss:338.565582275 Val loss: 336.49395752\n",
      "Epoch 3 Loss:329.755401611 Val loss: 332.890228271\n",
      "Epoch 4 Loss:325.595855713 Val loss: 330.201538086\n",
      "Epoch 5 Loss:322.133758545 Val loss: 327.636169434\n",
      "Epoch 6 Loss:318.826538086 Val loss: 325.007537842\n",
      "Epoch 7 Loss:315.438079834 Val loss: 322.236968994\n",
      "Epoch 8 Loss:311.884765625 Val loss: 319.339355469\n",
      "Epoch 9 Loss:308.202484131 Val loss: 316.386169434\n",
      "Epoch 10 Loss:304.47769165 Val loss: 313.45425415\n",
      "Epoch 11 Loss:300.838104248 Val loss: 310.655975342\n",
      "Epoch 12 Loss:297.382965088 Val loss: 308.04019165\n",
      "Epoch 13 Loss:294.142883301 Val loss: 305.648986816\n",
      "Epoch 14 Loss:291.186645508 Val loss: 303.549713135\n",
      "Epoch 15 Loss:288.550720215 Val loss: 301.76965332\n",
      "Epoch 16 Loss:286.249420166 Val loss: 300.323120117\n",
      "Epoch 17 Loss:284.273498535 Val loss: 299.147705078\n",
      "Epoch 18 Loss:282.56930542 Val loss: 298.184234619\n",
      "Epoch 19 Loss:281.077392578 Val loss: 297.373321533\n",
      "Epoch 20 Loss:279.74597168 Val loss: 296.670654297\n",
      "Epoch 21 Loss:278.533752441 Val loss: 296.035980225\n",
      "Epoch 22 Loss:277.413543701 Val loss: 295.446289062\n",
      "Epoch 23 Loss:276.36138916 Val loss: 294.889434814\n",
      "Epoch 24 Loss:275.363586426 Val loss: 294.35647583\n",
      "Epoch 25 Loss:274.411132812 Val loss: 293.834869385\n",
      "Epoch 26 Loss:273.49810791 Val loss: 293.329864502\n",
      "Epoch 27 Loss:272.619354248 Val loss: 292.838012695\n",
      "Epoch 28 Loss:271.770721436 Val loss: 292.361938477\n",
      "Epoch 29 Loss:270.95022583 Val loss: 291.901977539\n",
      "Epoch 30 Loss:270.151855469 Val loss: 291.452148438\n",
      "Epoch 31 Loss:269.370178223 Val loss: 291.007995605\n",
      "Epoch 32 Loss:268.604644775 Val loss: 290.569274902\n",
      "Epoch 33 Loss:267.850830078 Val loss: 290.144317627\n",
      "Epoch 34 Loss:267.108306885 Val loss: 289.730224609\n",
      "Epoch 35 Loss:266.378265381 Val loss: 289.317474365\n",
      "Epoch 36 Loss:265.659210205 Val loss: 288.909210205\n",
      "Epoch 37 Loss:264.95098877 Val loss: 288.500549316\n",
      "Epoch 38 Loss:264.251403809 Val loss: 288.097503662\n",
      "Epoch 39 Loss:263.562011719 Val loss: 287.704437256\n",
      "Epoch 40 Loss:262.88180542 Val loss: 287.324951172\n",
      "Epoch 41 Loss:262.208068848 Val loss: 286.949371338\n",
      "Epoch 42 Loss:261.538238525 Val loss: 286.57409668\n",
      "Epoch 43 Loss:260.86895752 Val loss: 286.203186035\n",
      "Epoch 44 Loss:260.199676514 Val loss: 285.830413818\n",
      "Epoch 45 Loss:259.529724121 Val loss: 285.464202881\n",
      "Epoch 46 Loss:258.859802246 Val loss: 285.10723877\n",
      "Epoch 47 Loss:258.191955566 Val loss: 284.761230469\n",
      "Epoch 48 Loss:257.528686523 Val loss: 284.417175293\n",
      "Epoch 49 Loss:256.86920166 Val loss: 284.071777344\n",
      "Epoch 50 Loss:256.212005615 Val loss: 283.732910156\n",
      "Epoch 51 Loss:255.557556152 Val loss: 283.394195557\n",
      "Epoch 52 Loss:254.906158447 Val loss: 283.050018311\n",
      "Epoch 53 Loss:254.25718689 Val loss: 282.70880127\n",
      "Epoch 54 Loss:253.609085083 Val loss: 282.373779297\n",
      "Epoch 55 Loss:252.961776733 Val loss: 282.03704834\n",
      "Epoch 56 Loss:252.319595337 Val loss: 281.703948975\n",
      "Epoch 57 Loss:251.68208313 Val loss: 281.380218506\n",
      "Epoch 58 Loss:251.048080444 Val loss: 281.065734863\n",
      "Epoch 59 Loss:250.418045044 Val loss: 280.758148193\n",
      "Epoch 60 Loss:249.789596558 Val loss: 280.458343506\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "fold_num = 1\n",
    "\n",
    "for train, test in kfold.split(encoded_X):\n",
    "   \n",
    "    print \"Fold {}\".format(fold_num)\n",
    "    \n",
    "    X_train, X_test = encoded_X[train], encoded_X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    # Create validation datasest\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)\n",
    "    \n",
    "    # Model\n",
    "    input_var = tf.placeholder(tf.float32, shape=[None, X_train.shape[1]])\n",
    "    gt_var = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    \n",
    "    model = tf.layers.dense(input_var, 100, activation=tf.nn.elu)\n",
    "    model = tf.layers.dense(model, 100, activation=tf.nn.elu)\n",
    "    output = tf.layers.dense(model, 1) \n",
    "    \n",
    "    # Loss function \n",
    "    loss = tf.reduce_mean(tf.losses.mean_squared_error(gt_var, output)**.5 )\n",
    "    opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "  \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(1, EPOCHS+1):  \n",
    "            epoch_loss = []\n",
    "            for X_batch, y_batch in batch_iterator(X_train, y_train, BATCH_SIZE):\n",
    "                batch_loss, pred, _ = sess.run([loss, output, opt], feed_dict={input_var: X_batch, gt_var: y_batch})                      \n",
    "                epoch_loss.append(batch_loss)                                \n",
    "             # Validation\n",
    "            val_loss = sess.run(loss, feed_dict={input_var: X_val, gt_var: y_val})\n",
    "                                               \n",
    "            print \"Epoch {} Loss:{} Val loss: {}\".format(epoch, np.mean(epoch_loss), val_loss)\n",
    "        fold_num += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# scorer = make_scorer(lambda a, b: mean_squared_error(a, b)**.5)\n",
    "# scores = cross_val_score(lr, encoded_X, y, cv=5, scoring=scorer, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340.449211318938"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 284.83019248\n"
     ]
    }
   ],
   "source": [
    "y_pred = cbr.fit(X_train, y_train, cat_features=CATEGORICAL_TEST_FEATURES_IDX).predict(X_test)\n",
    "print \"Mean squared error: {}\".format(mean_squared_error(y_test, y_pred)**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core._CatBoostBase at 0x7f8a7f594a50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make submission\n",
    "cbr.fit(X, y, cat_features=CATEGORICAL_TEST_FEATURES_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test = preprocess(test, CATEGORICAL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = test[TEST_FEATURES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions = cbr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 879.78794459, 1165.70020084,  279.39788828, ...,  154.47198918,\n",
       "        -62.34725293,   36.55400176])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_submission(ids, predictions):\n",
    "    df = pd.concat([ids, pd.Series(predictions)], axis=1)\n",
    "    return df.rename(columns={0: 'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l = test.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([test.id, pd.Series(predictions)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={0: 'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"catboost_d10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
