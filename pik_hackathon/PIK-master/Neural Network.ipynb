{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from preprocess import preprocess_train, preprocess, FEATURES, CATEGORICAL_FEATURES, TEST_FEATURES, CATEGORICAL_TEST_FEATURES_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y = preprocess_train(train, categotical_features=CATEGORICAL_FEATURES, features=TEST_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y)\n",
    "y = y[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8716, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def encode_categorical(X, cat_feat):\n",
    "    '''\n",
    "    Encodes categorical features with one-hot encoding and adds it into model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy.ndarray\n",
    "        Training features\n",
    "    cat_feat: list of int\n",
    "        Categorical features indices\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tweaked_X: numpy.ndarray\n",
    "        Tweaked X\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # All the rest\n",
    "    rest = np.ones(X.shape[1], np.bool)\n",
    "    rest[cat_feat] = False\n",
    "    \n",
    "    X_rest = X[:, rest]\n",
    "    \n",
    "    # Encoded\n",
    "    one_hot_encoded = []\n",
    "    \n",
    "    for col_idx in cat_feat:  \n",
    "        encoded = label_binarize(X[:, col_idx], np.unique(X[:, col_idx]).astype(int))\n",
    "        \n",
    "        #print encoded.shape\n",
    "        \n",
    "        one_hot_encoded.append(\n",
    "            encoded\n",
    "        )\n",
    "    \n",
    "    one_hot_encoded.append(X_rest)\n",
    "    \n",
    "    return np.hstack(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoded_X = encode_categorical(X, CATEGORICAL_TEST_FEATURES_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoded_X = scaler.fit_transform(encoded_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8716, 44) (8716, 46)\n"
     ]
    }
   ],
   "source": [
    "print X.shape, encoded_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batch_iterator(X, y, batch_size):\n",
    "    for i in range(0, len(X) - batch_size, batch_size):\n",
    "        yield X[i:i+batch_size], y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### CrossVal mean scores\n",
    "* catboost_d10: 224.01548277848525\n",
    "* catboost_d16: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training simple model\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1 Loss:383.995269775 Val loss: 347.532440186\n",
      "Epoch 2 Loss:341.911956787 Val loss: 337.476104736\n",
      "Epoch 3 Loss:334.416351318 Val loss: 330.11340332\n",
      "Epoch 4 Loss:326.847381592 Val loss: 321.784698486\n",
      "Epoch 5 Loss:318.927429199 Val loss: 314.336547852\n",
      "Epoch 6 Loss:311.939758301 Val loss: 308.616119385\n",
      "Epoch 7 Loss:305.973480225 Val loss: 303.845458984\n",
      "Epoch 8 Loss:300.771514893 Val loss: 299.565093994\n",
      "Epoch 9 Loss:296.221923828 Val loss: 295.799682617\n",
      "Epoch 10 Loss:292.042358398 Val loss: 292.37701416\n",
      "Epoch 11 Loss:288.230957031 Val loss: 289.510955811\n",
      "Epoch 12 Loss:284.77166748 Val loss: 286.966888428\n",
      "Epoch 13 Loss:281.52243042 Val loss: 284.697601318\n",
      "Epoch 14 Loss:278.445800781 Val loss: 282.68838501\n",
      "Epoch 15 Loss:275.51171875 Val loss: 280.936523438\n",
      "Epoch 16 Loss:272.685577393 Val loss: 279.291778564\n",
      "Epoch 17 Loss:270.029174805 Val loss: 277.783111572\n",
      "Epoch 18 Loss:267.564605713 Val loss: 276.409942627\n",
      "Epoch 19 Loss:265.238800049 Val loss: 275.252471924\n",
      "Epoch 20 Loss:263.054016113 Val loss: 274.256988525\n",
      "Epoch 21 Loss:260.893554688 Val loss: 273.355560303\n",
      "Epoch 22 Loss:258.812591553 Val loss: 272.581817627\n",
      "Epoch 23 Loss:256.840789795 Val loss: 271.837768555\n",
      "Epoch 24 Loss:254.939758301 Val loss: 271.25491333\n",
      "Epoch 25 Loss:253.042556763 Val loss: 270.612457275\n",
      "Epoch 26 Loss:251.158447266 Val loss: 270.167022705\n",
      "Epoch 27 Loss:249.32522583 Val loss: 269.690612793\n",
      "Epoch 28 Loss:247.558425903 Val loss: 269.30255127\n",
      "Epoch 29 Loss:245.840255737 Val loss: 268.943786621\n",
      "Epoch 30 Loss:244.13899231 Val loss: 268.668457031\n",
      "Epoch 31 Loss:242.419906616 Val loss: 268.321350098\n",
      "Epoch 32 Loss:240.73274231 Val loss: 268.122589111\n",
      "Epoch 33 Loss:239.108840942 Val loss: 267.906799316\n",
      "Epoch 34 Loss:237.428970337 Val loss: 267.765136719\n",
      "Epoch 35 Loss:235.830383301 Val loss: 267.629577637\n",
      "Epoch 36 Loss:234.181152344 Val loss: 267.706481934\n",
      "Epoch 37 Loss:232.587036133 Val loss: 267.749053955\n",
      "Epoch 38 Loss:231.054489136 Val loss: 267.865753174\n",
      "Epoch 39 Loss:229.53302002 Val loss: 267.943878174\n",
      "Epoch 40 Loss:228.139251709 Val loss: 268.063201904\n",
      "Epoch 41 Loss:226.812591553 Val loss: 268.256225586\n",
      "Epoch 42 Loss:225.527923584 Val loss: 268.451019287\n",
      "Epoch 43 Loss:224.292938232 Val loss: 268.578491211\n",
      "Epoch 44 Loss:223.09135437 Val loss: 268.636260986\n",
      "Epoch 45 Loss:221.887420654 Val loss: 268.830780029\n",
      "Epoch 46 Loss:220.69102478 Val loss: 268.985321045\n",
      "Epoch 47 Loss:219.489990234 Val loss: 269.07144165\n",
      "Epoch 48 Loss:218.317703247 Val loss: 269.207244873\n",
      "Epoch 49 Loss:217.164932251 Val loss: 269.242553711\n",
      "Epoch 50 Loss:216.060577393 Val loss: 269.285125732\n",
      "Epoch 51 Loss:214.939727783 Val loss: 269.247497559\n",
      "Epoch 52 Loss:213.828094482 Val loss: 269.254638672\n",
      "Epoch 53 Loss:212.753036499 Val loss: 269.163879395\n",
      "Epoch 54 Loss:211.643829346 Val loss: 269.162689209\n",
      "Epoch 55 Loss:210.550323486 Val loss: 269.13873291\n",
      "Epoch 56 Loss:209.495849609 Val loss: 269.09954834\n",
      "Epoch 57 Loss:208.43409729 Val loss: 269.042663574\n",
      "Epoch 58 Loss:207.401504517 Val loss: 269.084777832\n",
      "Epoch 59 Loss:206.348907471 Val loss: 269.027587891\n",
      "Epoch 60 Loss:205.356933594 Val loss: 269.190673828\n",
      "Epoch 61 Loss:204.279693604 Val loss: 269.133300781\n",
      "Epoch 62 Loss:203.210601807 Val loss: 269.374176025\n",
      "Epoch 63 Loss:202.144317627 Val loss: 269.306671143\n",
      "Epoch 64 Loss:201.080612183 Val loss: 269.399169922\n",
      "Epoch 65 Loss:200.021713257 Val loss: 269.166748047\n",
      "Epoch 66 Loss:198.970123291 Val loss: 269.085540771\n",
      "Epoch 67 Loss:197.922042847 Val loss: 268.98046875\n",
      "Epoch 68 Loss:196.942459106 Val loss: 268.77243042\n",
      "Epoch 69 Loss:195.880157471 Val loss: 268.77053833\n",
      "Epoch 70 Loss:194.965591431 Val loss: 268.690093994\n",
      "Epoch 71 Loss:193.846572876 Val loss: 268.776000977\n",
      "Epoch 72 Loss:192.925231934 Val loss: 268.649810791\n",
      "Epoch 73 Loss:191.786911011 Val loss: 268.826202393\n",
      "Epoch 74 Loss:190.918182373 Val loss: 268.589355469\n",
      "Epoch 75 Loss:189.825302124 Val loss: 268.846313477\n",
      "Epoch 76 Loss:188.934341431 Val loss: 268.477630615\n",
      "Epoch 77 Loss:187.8309021 Val loss: 268.530059814\n",
      "Epoch 78 Loss:186.998016357 Val loss: 268.240631104\n",
      "Epoch 79 Loss:185.873321533 Val loss: 268.341217041\n",
      "Epoch 80 Loss:185.003753662 Val loss: 268.138519287\n",
      "Epoch 81 Loss:183.824462891 Val loss: 268.201812744\n",
      "Epoch 82 Loss:182.939651489 Val loss: 267.980499268\n",
      "Epoch 83 Loss:181.908447266 Val loss: 267.878723145\n",
      "Epoch 84 Loss:180.994918823 Val loss: 267.627593994\n",
      "Epoch 85 Loss:179.978103638 Val loss: 267.515777588\n",
      "Epoch 86 Loss:179.133361816 Val loss: 267.408721924\n",
      "Epoch 87 Loss:178.230834961 Val loss: 267.410980225\n",
      "Epoch 88 Loss:177.441070557 Val loss: 267.200958252\n",
      "Epoch 89 Loss:176.536468506 Val loss: 267.348022461\n",
      "Epoch 90 Loss:175.859832764 Val loss: 267.157348633\n",
      "Epoch 91 Loss:175.042327881 Val loss: 267.446716309\n",
      "Epoch 92 Loss:174.436676025 Val loss: 267.240112305\n",
      "Epoch 93 Loss:173.641677856 Val loss: 267.528045654\n",
      "Epoch 94 Loss:172.964630127 Val loss: 267.30758667\n",
      "Epoch 95 Loss:172.272796631 Val loss: 267.518280029\n",
      "Epoch 96 Loss:171.705337524 Val loss: 267.526702881\n",
      "Epoch 97 Loss:170.889328003 Val loss: 267.768768311\n",
      "Epoch 98 Loss:170.345123291 Val loss: 267.751251221\n",
      "Epoch 99 Loss:169.441818237 Val loss: 268.020721436\n",
      "Epoch 100 Loss:168.864761353 Val loss: 268.060577393\n",
      "Epoch 101 Loss:167.965545654 Val loss: 267.921600342\n",
      "Epoch 102 Loss:167.419052124 Val loss: 268.089935303\n",
      "Epoch 103 Loss:166.481185913 Val loss: 267.859375\n",
      "Epoch 104 Loss:165.856567383 Val loss: 268.149139404\n",
      "Epoch 105 Loss:164.9269104 Val loss: 267.949066162\n",
      "Epoch 106 Loss:164.47013855 Val loss: 268.404907227\n",
      "Epoch 107 Loss:163.549102783 Val loss: 268.156158447\n",
      "Epoch 108 Loss:163.093948364 Val loss: 268.467437744\n",
      "Epoch 109 Loss:162.081741333 Val loss: 268.282318115\n",
      "Epoch 110 Loss:161.767440796 Val loss: 268.806030273\n",
      "Epoch 111 Loss:160.72769165 Val loss: 268.695495605\n",
      "Epoch 112 Loss:160.360122681 Val loss: 269.143829346\n",
      "Epoch 113 Loss:159.501342773 Val loss: 269.190185547\n",
      "Epoch 114 Loss:159.158096313 Val loss: 269.70803833\n",
      "Epoch 115 Loss:158.298187256 Val loss: 269.714324951\n",
      "Epoch 116 Loss:158.101150513 Val loss: 270.530059814\n",
      "Epoch 117 Loss:157.31918335 Val loss: 270.494689941\n",
      "Epoch 118 Loss:156.930496216 Val loss: 270.584960938\n",
      "Epoch 119 Loss:156.177581787 Val loss: 270.960571289\n",
      "Epoch 120 Loss:156.36265564 Val loss: 269.758575439\n",
      "Epoch 121 Loss:155.676315308 Val loss: 270.92364502\n",
      "Epoch 122 Loss:155.084396362 Val loss: 269.578430176\n",
      "Epoch 123 Loss:154.193466187 Val loss: 270.628662109\n",
      "Epoch 124 Loss:152.943878174 Val loss: 271.006866455\n",
      "Epoch 125 Loss:152.985626221 Val loss: 270.889038086\n",
      "Epoch 126 Loss:153.008392334 Val loss: 270.445709229\n",
      "Epoch 127 Loss:152.265853882 Val loss: 267.555053711\n",
      "Epoch 128 Loss:153.983825684 Val loss: 269.84185791\n",
      "Epoch 129 Loss:152.098800659 Val loss: 269.643493652\n",
      "Epoch 130 Loss:150.642547607 Val loss: 268.754516602\n",
      "Epoch 131 Loss:150.715087891 Val loss: 271.046325684\n",
      "Epoch 132 Loss:149.410858154 Val loss: 270.534393311\n",
      "Epoch 133 Loss:151.379699707 Val loss: 267.359191895\n",
      "Epoch 134 Loss:150.63760376 Val loss: 268.703613281\n",
      "Epoch 135 Loss:148.549743652 Val loss: 269.150756836\n",
      "Epoch 136 Loss:147.718276978 Val loss: 267.723327637\n",
      "Epoch 137 Loss:148.718826294 Val loss: 268.491546631\n",
      "Epoch 138 Loss:146.309249878 Val loss: 268.375579834\n",
      "Epoch 139 Loss:149.002120972 Val loss: 270.207946777\n",
      "Epoch 140 Loss:145.36730957 Val loss: 270.648803711\n",
      "Epoch 141 Loss:147.577987671 Val loss: 268.957061768\n",
      "Epoch 142 Loss:148.417663574 Val loss: 271.922058105\n",
      "Epoch 143 Loss:146.715484619 Val loss: 267.575500488\n",
      "Epoch 144 Loss:143.829376221 Val loss: 268.740539551\n",
      "Epoch 145 Loss:142.865936279 Val loss: 268.25680542\n",
      "Epoch 146 Loss:141.800811768 Val loss: 269.683746338\n",
      "Epoch 147 Loss:142.517669678 Val loss: 270.16317749\n",
      "Epoch 148 Loss:142.257629395 Val loss: 270.922698975\n",
      "Epoch 149 Loss:141.495803833 Val loss: 270.881134033\n",
      "Epoch 150 Loss:141.809020996 Val loss: 269.693359375\n",
      "Epoch 151 Loss:141.487915039 Val loss: 269.766357422\n",
      "Epoch 152 Loss:140.474349976 Val loss: 271.413970947\n",
      "Epoch 153 Loss:138.466644287 Val loss: 270.423095703\n",
      "Epoch 154 Loss:142.698684692 Val loss: 272.415588379\n",
      "Epoch 155 Loss:139.64263916 Val loss: 269.471954346\n",
      "Epoch 156 Loss:142.516479492 Val loss: 271.380523682\n",
      "Epoch 157 Loss:139.060684204 Val loss: 270.184509277\n",
      "Epoch 158 Loss:137.407562256 Val loss: 271.011169434\n",
      "Epoch 159 Loss:137.099639893 Val loss: 271.712158203\n",
      "Epoch 160 Loss:138.466384888 Val loss: 269.666595459\n",
      "Epoch 161 Loss:137.807678223 Val loss: 269.948486328\n",
      "Epoch 162 Loss:136.743392944 Val loss: 273.149871826\n",
      "Epoch 163 Loss:136.961013794 Val loss: 270.145996094\n",
      "Epoch 164 Loss:137.879241943 Val loss: 268.530792236\n",
      "Epoch 165 Loss:136.72845459 Val loss: 267.675354004\n",
      "Epoch 166 Loss:136.641204834 Val loss: 267.151275635\n",
      "Epoch 167 Loss:136.395278931 Val loss: 268.983734131\n",
      "Epoch 168 Loss:138.162597656 Val loss: 267.46383667\n",
      "Epoch 169 Loss:136.795028687 Val loss: 267.000396729\n",
      "Epoch 170 Loss:135.870040894 Val loss: 264.784912109\n",
      "Epoch 171 Loss:135.613769531 Val loss: 265.165222168\n",
      "Epoch 172 Loss:135.486938477 Val loss: 264.485137939\n",
      "Epoch 173 Loss:137.337844849 Val loss: 267.484008789\n",
      "Epoch 174 Loss:137.96395874 Val loss: 266.359741211\n",
      "Epoch 175 Loss:135.141342163 Val loss: 265.108673096\n",
      "Epoch 176 Loss:134.795730591 Val loss: 267.160369873\n",
      "Epoch 177 Loss:131.79725647 Val loss: 264.787536621\n",
      "Epoch 178 Loss:131.460037231 Val loss: 266.747009277\n",
      "Epoch 179 Loss:133.424041748 Val loss: 266.85043335\n",
      "Epoch 180 Loss:132.303634644 Val loss: 267.315948486\n",
      "Epoch 181 Loss:133.482940674 Val loss: 268.243469238\n",
      "Epoch 182 Loss:131.925476074 Val loss: 271.322814941\n",
      "Epoch 183 Loss:131.879577637 Val loss: 264.805328369\n",
      "Epoch 184 Loss:131.84161377 Val loss: 269.666564941\n",
      "Epoch 185 Loss:130.650863647 Val loss: 267.360656738\n",
      "Epoch 186 Loss:130.427963257 Val loss: 271.281921387\n",
      "Epoch 187 Loss:133.589202881 Val loss: 269.993286133\n",
      "Epoch 188 Loss:131.402587891 Val loss: 269.88760376\n",
      "Epoch 189 Loss:130.159576416 Val loss: 268.615203857\n",
      "Epoch 190 Loss:129.888031006 Val loss: 269.16619873\n",
      "Epoch 191 Loss:130.583236694 Val loss: 272.126464844\n",
      "Epoch 192 Loss:127.834274292 Val loss: 269.516082764\n",
      "Epoch 193 Loss:127.65057373 Val loss: 268.824645996\n",
      "Epoch 194 Loss:131.383514404 Val loss: 272.478912354\n",
      "Epoch 195 Loss:127.842811584 Val loss: 270.397644043\n",
      "Epoch 196 Loss:128.802368164 Val loss: 270.382171631\n",
      "Epoch 197 Loss:126.80380249 Val loss: 271.658782959\n",
      "Epoch 198 Loss:125.56035614 Val loss: 272.380737305\n",
      "Epoch 199 Loss:126.917175293 Val loss: 269.833862305\n",
      "Epoch 200 Loss:126.676353455 Val loss: 271.970336914\n",
      "Epoch 201 Loss:128.60484314 Val loss: 270.889465332\n",
      "Epoch 202 Loss:124.14528656 Val loss: 272.175964355\n",
      "Epoch 203 Loss:123.381614685 Val loss: 271.544372559\n",
      "Epoch 204 Loss:124.848686218 Val loss: 274.789703369\n",
      "Epoch 205 Loss:127.327301025 Val loss: 272.718383789\n",
      "Epoch 206 Loss:124.213096619 Val loss: 271.043548584\n",
      "Epoch 207 Loss:125.107528687 Val loss: 275.686279297\n",
      "Epoch 208 Loss:127.046524048 Val loss: 272.07409668\n",
      "Epoch 209 Loss:123.410087585 Val loss: 273.42565918\n",
      "Epoch 210 Loss:123.56363678 Val loss: 271.842437744\n",
      "Epoch 211 Loss:123.046585083 Val loss: 270.734130859\n",
      "Epoch 212 Loss:123.327430725 Val loss: 270.436706543\n",
      "Epoch 213 Loss:120.426055908 Val loss: 269.309295654\n",
      "Epoch 214 Loss:120.110107422 Val loss: 273.470733643\n",
      "Epoch 215 Loss:123.621055603 Val loss: 273.69921875\n",
      "Epoch 216 Loss:121.200042725 Val loss: 269.942901611\n",
      "Epoch 217 Loss:119.10697937 Val loss: 272.383666992\n",
      "Epoch 218 Loss:123.272911072 Val loss: 269.531768799\n",
      "Epoch 219 Loss:121.182540894 Val loss: 269.604888916\n",
      "Epoch 220 Loss:120.821662903 Val loss: 268.886077881\n",
      "Epoch 221 Loss:122.349525452 Val loss: 271.206207275\n",
      "Epoch 222 Loss:121.277580261 Val loss: 269.695343018\n",
      "Epoch 223 Loss:119.705688477 Val loss: 267.922515869\n",
      "Epoch 224 Loss:121.736961365 Val loss: 267.861694336\n",
      "Epoch 225 Loss:117.637054443 Val loss: 269.268859863\n",
      "Epoch 226 Loss:116.16633606 Val loss: 270.07824707\n",
      "Epoch 227 Loss:115.453979492 Val loss: 269.85534668\n",
      "Epoch 228 Loss:116.991119385 Val loss: 271.050750732\n",
      "Epoch 229 Loss:117.254478455 Val loss: 273.049682617\n",
      "Epoch 230 Loss:115.15586853 Val loss: 271.293151855\n",
      "Epoch 231 Loss:114.948310852 Val loss: 271.048492432\n",
      "Epoch 232 Loss:114.33895874 Val loss: 271.733459473\n",
      "Epoch 233 Loss:113.522239685 Val loss: 272.288299561\n",
      "Epoch 234 Loss:116.13168335 Val loss: 275.11517334\n",
      "Epoch 235 Loss:115.149299622 Val loss: 272.29006958\n",
      "Epoch 236 Loss:115.241012573 Val loss: 276.624938965\n",
      "Epoch 237 Loss:116.464660645 Val loss: 271.876159668\n",
      "Epoch 238 Loss:113.101776123 Val loss: 273.146362305\n",
      "Epoch 239 Loss:113.873199463 Val loss: 272.362304688\n",
      "Epoch 240 Loss:113.831802368 Val loss: 274.341796875\n",
      "Epoch 241 Loss:113.06855011 Val loss: 274.409759521\n",
      "Epoch 242 Loss:113.272865295 Val loss: 275.036407471\n",
      "Epoch 243 Loss:114.746574402 Val loss: 274.739929199\n",
      "Epoch 244 Loss:114.534255981 Val loss: 275.597747803\n",
      "Epoch 245 Loss:115.955688477 Val loss: 278.793548584\n",
      "Epoch 246 Loss:117.202476501 Val loss: 268.226867676\n",
      "Epoch 247 Loss:113.723831177 Val loss: 274.695007324\n",
      "Epoch 248 Loss:112.85748291 Val loss: 270.632537842\n",
      "Epoch 249 Loss:111.822097778 Val loss: 272.342926025\n",
      "Epoch 250 Loss:111.54083252 Val loss: 270.647888184\n",
      "Epoch 251 Loss:110.131904602 Val loss: 274.090515137\n",
      "Epoch 252 Loss:109.953147888 Val loss: 267.754638672\n",
      "Epoch 253 Loss:108.467910767 Val loss: 271.365905762\n",
      "Epoch 254 Loss:108.765830994 Val loss: 275.023162842\n",
      "Epoch 255 Loss:107.81602478 Val loss: 277.957183838\n",
      "Epoch 256 Loss:108.754508972 Val loss: 274.437469482\n",
      "Epoch 257 Loss:112.669250488 Val loss: 273.681304932\n",
      "Epoch 258 Loss:109.961372375 Val loss: 274.132446289\n",
      "Epoch 259 Loss:110.981765747 Val loss: 275.321716309\n",
      "Epoch 260 Loss:111.493675232 Val loss: 273.505584717\n",
      "Epoch 261 Loss:109.070075989 Val loss: 279.034881592\n",
      "Epoch 262 Loss:105.91468811 Val loss: 277.208374023\n",
      "Epoch 263 Loss:109.896278381 Val loss: 272.913543701\n",
      "Epoch 264 Loss:108.437767029 Val loss: 274.558990479\n",
      "Epoch 265 Loss:109.118766785 Val loss: 282.45111084\n",
      "Epoch 266 Loss:108.046302795 Val loss: 277.7371521\n",
      "Epoch 267 Loss:105.00894928 Val loss: 277.590789795\n",
      "Epoch 268 Loss:106.197875977 Val loss: 275.239624023\n",
      "Epoch 269 Loss:106.942390442 Val loss: 276.863647461\n",
      "Epoch 270 Loss:107.183929443 Val loss: 280.9480896\n",
      "Epoch 271 Loss:106.131866455 Val loss: 279.776947021\n",
      "Epoch 272 Loss:107.090942383 Val loss: 281.259277344\n",
      "Epoch 273 Loss:107.142280579 Val loss: 282.351959229\n",
      "Epoch 274 Loss:107.633674622 Val loss: 281.336730957\n",
      "Epoch 275 Loss:107.706794739 Val loss: 279.483795166\n",
      "Epoch 276 Loss:106.254364014 Val loss: 285.403778076\n",
      "Epoch 277 Loss:105.82093811 Val loss: 281.496307373\n",
      "Epoch 278 Loss:105.149871826 Val loss: 284.003692627\n",
      "Epoch 279 Loss:106.487205505 Val loss: 278.528900146\n",
      "Epoch 280 Loss:102.627075195 Val loss: 282.945709229\n",
      "Epoch 281 Loss:103.792228699 Val loss: 279.868133545\n",
      "Epoch 282 Loss:106.68536377 Val loss: 287.249450684\n",
      "Epoch 283 Loss:103.855110168 Val loss: 283.22668457\n",
      "Epoch 284 Loss:102.676139832 Val loss: 287.830444336\n",
      "Epoch 285 Loss:101.868041992 Val loss: 282.735961914\n",
      "Epoch 286 Loss:103.189224243 Val loss: 286.985626221\n",
      "Epoch 287 Loss:101.23298645 Val loss: 277.85635376\n",
      "Epoch 288 Loss:100.599403381 Val loss: 286.121948242\n",
      "Epoch 289 Loss:99.794960022 Val loss: 284.459075928\n",
      "Epoch 290 Loss:97.5174560547 Val loss: 286.50793457\n",
      "Epoch 291 Loss:105.269767761 Val loss: 285.265808105\n",
      "Epoch 292 Loss:102.950317383 Val loss: 285.016204834\n",
      "Epoch 293 Loss:100.738929749 Val loss: 287.060394287\n",
      "Epoch 294 Loss:100.8203125 Val loss: 285.232086182\n",
      "Epoch 295 Loss:98.9685974121 Val loss: 284.60848999\n",
      "Epoch 296 Loss:98.2942657471 Val loss: 288.274993896\n",
      "Epoch 297 Loss:100.427146912 Val loss: 292.90222168\n",
      "Epoch 298 Loss:99.5024032593 Val loss: 284.3309021\n",
      "Epoch 299 Loss:98.8231735229 Val loss: 290.574371338\n",
      "Epoch 300 Loss:97.30493927 Val loss: 286.365142822\n",
      "Epoch 301 Loss:100.633750916 Val loss: 286.369476318\n",
      "Epoch 302 Loss:98.7620697021 Val loss: 282.873016357\n",
      "Epoch 303 Loss:98.0303649902 Val loss: 294.387908936\n",
      "Epoch 304 Loss:103.46144104 Val loss: 284.044494629\n",
      "Epoch 305 Loss:97.2988357544 Val loss: 289.917938232\n",
      "Epoch 306 Loss:97.0639266968 Val loss: 286.324859619\n",
      "Epoch 307 Loss:98.8822021484 Val loss: 281.684844971\n",
      "Epoch 308 Loss:97.9645080566 Val loss: 285.339019775\n",
      "Epoch 309 Loss:95.4067687988 Val loss: 293.573638916\n",
      "Epoch 310 Loss:96.6423110962 Val loss: 287.777618408\n",
      "Epoch 311 Loss:99.8425598145 Val loss: 287.136535645\n",
      "Epoch 312 Loss:96.9398880005 Val loss: 285.2862854\n",
      "Epoch 313 Loss:99.4507980347 Val loss: 286.727203369\n",
      "Epoch 314 Loss:92.7044677734 Val loss: 287.892181396\n",
      "Epoch 315 Loss:95.9554595947 Val loss: 287.862518311\n",
      "Epoch 316 Loss:93.9816818237 Val loss: 288.537902832\n",
      "Epoch 317 Loss:99.6590499878 Val loss: 284.867095947\n",
      "Epoch 318 Loss:94.6608963013 Val loss: 285.43145752\n",
      "Epoch 319 Loss:94.5767974854 Val loss: 285.577850342\n",
      "Epoch 320 Loss:92.6751251221 Val loss: 285.64654541\n",
      "Epoch 321 Loss:92.4632415771 Val loss: 286.841033936\n",
      "Epoch 322 Loss:91.3276672363 Val loss: 291.0859375\n",
      "Epoch 323 Loss:92.649848938 Val loss: 283.184204102\n",
      "Epoch 324 Loss:91.6404495239 Val loss: 284.046478271\n",
      "Epoch 325 Loss:93.4888687134 Val loss: 289.917633057\n",
      "Epoch 326 Loss:94.0245666504 Val loss: 284.299499512\n",
      "Epoch 327 Loss:96.5155029297 Val loss: 287.349700928\n",
      "Epoch 328 Loss:96.3622360229 Val loss: 287.98614502\n",
      "Epoch 329 Loss:92.5867996216 Val loss: 285.630828857\n",
      "Epoch 330 Loss:91.4035491943 Val loss: 285.397094727\n",
      "Epoch 331 Loss:92.4253234863 Val loss: 288.743865967\n",
      "Epoch 332 Loss:93.6862869263 Val loss: 286.097167969\n",
      "Epoch 333 Loss:93.5759048462 Val loss: 291.071502686\n",
      "Epoch 334 Loss:93.6658248901 Val loss: 285.956542969\n",
      "Epoch 335 Loss:92.5177383423 Val loss: 287.196105957\n",
      "Epoch 336 Loss:89.6167755127 Val loss: 284.352661133\n",
      "Epoch 337 Loss:91.1989212036 Val loss: 295.571868896\n",
      "Epoch 338 Loss:92.4476852417 Val loss: 287.591674805\n",
      "Epoch 339 Loss:91.5893783569 Val loss: 293.208648682\n",
      "Epoch 340 Loss:90.2471618652 Val loss: 286.701965332\n",
      "Epoch 341 Loss:89.4973754883 Val loss: 289.173034668\n",
      "Epoch 342 Loss:89.435256958 Val loss: 289.897125244\n",
      "Epoch 343 Loss:89.6212615967 Val loss: 288.609039307\n",
      "Epoch 344 Loss:88.4458007812 Val loss: 288.822418213\n",
      "Epoch 345 Loss:89.3324050903 Val loss: 290.118988037\n",
      "Epoch 346 Loss:90.7031478882 Val loss: 293.743103027\n",
      "Epoch 347 Loss:92.1322402954 Val loss: 287.580566406\n",
      "Epoch 348 Loss:89.7518463135 Val loss: 293.551239014\n",
      "Epoch 349 Loss:89.5908355713 Val loss: 292.380462646\n",
      "Epoch 350 Loss:89.4370803833 Val loss: 293.02645874\n",
      "Epoch 351 Loss:88.6576843262 Val loss: 290.33996582\n",
      "Epoch 352 Loss:89.593536377 Val loss: 284.131469727\n",
      "Epoch 353 Loss:87.6331481934 Val loss: 288.683319092\n",
      "Epoch 354 Loss:89.7833480835 Val loss: 285.235595703\n",
      "Epoch 355 Loss:86.9356155396 Val loss: 287.039245605\n",
      "Epoch 356 Loss:89.1142730713 Val loss: 289.270233154\n",
      "Epoch 357 Loss:87.3519287109 Val loss: 293.5362854\n",
      "Epoch 358 Loss:90.0164794922 Val loss: 290.233428955\n",
      "Epoch 359 Loss:88.0972976685 Val loss: 292.818054199\n",
      "Epoch 360 Loss:88.0860290527 Val loss: 292.267120361\n",
      "Epoch 361 Loss:88.3680801392 Val loss: 293.48260498\n",
      "Epoch 362 Loss:86.9236068726 Val loss: 291.722503662\n",
      "Epoch 363 Loss:86.0211486816 Val loss: 290.591430664\n",
      "Epoch 364 Loss:85.7767944336 Val loss: 286.502593994\n",
      "Epoch 365 Loss:88.4208679199 Val loss: 293.186553955\n",
      "Epoch 366 Loss:87.0950775146 Val loss: 290.595367432\n",
      "Epoch 367 Loss:87.104927063 Val loss: 292.328979492\n",
      "Epoch 368 Loss:90.4752578735 Val loss: 291.818206787\n",
      "Epoch 369 Loss:86.4928436279 Val loss: 291.216308594\n",
      "Epoch 370 Loss:85.0968093872 Val loss: 291.761535645\n",
      "Epoch 371 Loss:85.0950698853 Val loss: 291.173522949\n",
      "Epoch 372 Loss:85.0602264404 Val loss: 289.991119385\n",
      "Epoch 373 Loss:85.2265396118 Val loss: 292.084655762\n",
      "Epoch 374 Loss:88.085395813 Val loss: 285.817169189\n",
      "Epoch 375 Loss:87.4939880371 Val loss: 286.488739014\n",
      "Epoch 376 Loss:86.8841552734 Val loss: 289.061157227\n",
      "Epoch 377 Loss:84.9824066162 Val loss: 294.12979126\n",
      "Epoch 378 Loss:84.9926986694 Val loss: 289.310211182\n",
      "Epoch 379 Loss:85.4347229004 Val loss: 293.754730225\n",
      "Epoch 380 Loss:86.0347366333 Val loss: 288.8956604\n",
      "Epoch 381 Loss:83.2453536987 Val loss: 288.658233643\n",
      "Epoch 382 Loss:83.8979187012 Val loss: 287.796295166\n",
      "Epoch 383 Loss:84.6242218018 Val loss: 292.686676025\n",
      "Epoch 384 Loss:85.2840118408 Val loss: 293.956268311\n",
      "Epoch 385 Loss:84.8000564575 Val loss: 292.539520264\n",
      "Epoch 386 Loss:84.4647598267 Val loss: 293.637908936\n",
      "Epoch 387 Loss:82.8266143799 Val loss: 289.847564697\n",
      "Epoch 388 Loss:82.8345336914 Val loss: 287.381378174\n",
      "Epoch 389 Loss:85.1121826172 Val loss: 288.968536377\n",
      "Epoch 390 Loss:84.5648117065 Val loss: 295.423278809\n",
      "Epoch 391 Loss:84.6067504883 Val loss: 292.417358398\n",
      "Epoch 392 Loss:84.230430603 Val loss: 288.865722656\n",
      "Epoch 393 Loss:84.987953186 Val loss: 293.566345215\n",
      "Epoch 394 Loss:83.0411376953 Val loss: 293.732299805\n",
      "Epoch 395 Loss:84.3255004883 Val loss: 294.827026367\n",
      "Epoch 396 Loss:82.1803970337 Val loss: 290.664978027\n",
      "Epoch 397 Loss:86.8977508545 Val loss: 288.296875\n",
      "Epoch 398 Loss:84.4326019287 Val loss: 289.902130127\n",
      "Epoch 399 Loss:82.9460525513 Val loss: 290.350463867\n",
      "Epoch 400 Loss:80.5654983521 Val loss: 291.813354492\n",
      "Epoch 401 Loss:86.0968551636 Val loss: 285.314483643\n",
      "Epoch 402 Loss:82.1068344116 Val loss: 295.85748291\n",
      "Epoch 403 Loss:82.0820159912 Val loss: 290.819641113\n",
      "Epoch 404 Loss:79.8277053833 Val loss: 289.028045654\n",
      "Epoch 405 Loss:82.3722076416 Val loss: 290.552368164\n",
      "Epoch 406 Loss:84.2997055054 Val loss: 292.913238525\n",
      "Epoch 407 Loss:82.7300643921 Val loss: 298.887908936\n",
      "Epoch 408 Loss:80.3010482788 Val loss: 297.039794922\n",
      "Epoch 409 Loss:82.4150466919 Val loss: 294.081970215\n",
      "Epoch 410 Loss:83.3172454834 Val loss: 292.798034668\n",
      "Epoch 411 Loss:81.0755310059 Val loss: 295.43927002\n",
      "Epoch 412 Loss:82.9307632446 Val loss: 295.763000488\n",
      "Epoch 413 Loss:82.803024292 Val loss: 292.24810791\n",
      "Epoch 414 Loss:83.5484313965 Val loss: 288.985229492\n",
      "Epoch 415 Loss:80.7517471313 Val loss: 293.628753662\n",
      "Epoch 416 Loss:81.6340255737 Val loss: 293.830505371\n",
      "Epoch 417 Loss:81.5164413452 Val loss: 291.011871338\n",
      "Epoch 418 Loss:79.9748153687 Val loss: 293.147003174\n",
      "Epoch 419 Loss:79.8430633545 Val loss: 291.919036865\n",
      "Epoch 420 Loss:78.2311172485 Val loss: 290.082885742\n",
      "Epoch 421 Loss:82.2569732666 Val loss: 288.62097168\n",
      "Epoch 422 Loss:81.4331817627 Val loss: 290.714996338\n",
      "Epoch 423 Loss:80.0169754028 Val loss: 294.504272461\n",
      "Epoch 424 Loss:80.1261367798 Val loss: 292.626708984\n",
      "Epoch 425 Loss:79.8372650146 Val loss: 294.820892334\n",
      "Epoch 426 Loss:82.1124572754 Val loss: 294.507598877\n",
      "Epoch 427 Loss:78.8856658936 Val loss: 295.341705322\n",
      "Epoch 428 Loss:79.0766830444 Val loss: 296.868896484\n",
      "Epoch 429 Loss:78.6821670532 Val loss: 296.925018311\n",
      "Epoch 430 Loss:77.3490905762 Val loss: 299.325653076\n",
      "Epoch 431 Loss:80.7609786987 Val loss: 296.570739746\n",
      "Epoch 432 Loss:78.5353622437 Val loss: 295.5496521\n",
      "Epoch 433 Loss:80.285949707 Val loss: 296.554443359\n",
      "Epoch 434 Loss:78.2053527832 Val loss: 294.231140137\n",
      "Epoch 435 Loss:79.4126968384 Val loss: 297.51184082\n",
      "Epoch 436 Loss:79.8866195679 Val loss: 298.024017334\n",
      "Epoch 437 Loss:80.9875869751 Val loss: 295.587677002\n",
      "Epoch 438 Loss:81.7980880737 Val loss: 293.418334961\n",
      "Epoch 439 Loss:78.2501220703 Val loss: 291.504730225\n",
      "Epoch 440 Loss:76.4046478271 Val loss: 295.120269775\n",
      "Epoch 441 Loss:78.7227401733 Val loss: 299.106079102\n",
      "Epoch 442 Loss:78.6582107544 Val loss: 294.949249268\n",
      "Epoch 443 Loss:75.8167037964 Val loss: 296.210723877\n",
      "Epoch 444 Loss:77.1062164307 Val loss: 295.997436523\n",
      "Epoch 445 Loss:79.2657318115 Val loss: 295.354980469\n",
      "Epoch 446 Loss:77.4931716919 Val loss: 293.003356934\n",
      "Epoch 447 Loss:78.9978637695 Val loss: 289.39743042\n",
      "Epoch 448 Loss:77.8930664062 Val loss: 291.283660889\n",
      "Epoch 449 Loss:77.967338562 Val loss: 290.790618896\n",
      "Epoch 450 Loss:77.2004318237 Val loss: 298.137023926\n",
      "Epoch 451 Loss:76.2410583496 Val loss: 298.69744873\n",
      "Epoch 452 Loss:75.8942947388 Val loss: 297.28036499\n",
      "Epoch 453 Loss:80.4530029297 Val loss: 295.096832275\n",
      "Epoch 454 Loss:79.8181838989 Val loss: 295.519500732\n",
      "Epoch 455 Loss:77.5829696655 Val loss: 293.643035889\n",
      "Epoch 456 Loss:75.8651733398 Val loss: 292.569671631\n",
      "Epoch 457 Loss:76.1480865479 Val loss: 294.762664795\n",
      "Epoch 458 Loss:75.9074783325 Val loss: 295.222381592\n",
      "Epoch 459 Loss:75.0572738647 Val loss: 297.586853027\n",
      "Epoch 460 Loss:78.0425872803 Val loss: 295.257110596\n",
      "Epoch 461 Loss:76.2640228271 Val loss: 293.481018066\n",
      "Epoch 462 Loss:75.2630004883 Val loss: 292.198577881\n",
      "Epoch 463 Loss:76.8005981445 Val loss: 291.805114746\n",
      "Epoch 464 Loss:79.0405731201 Val loss: 294.081665039\n",
      "Epoch 465 Loss:77.1828765869 Val loss: 301.849731445\n",
      "Epoch 466 Loss:76.8993225098 Val loss: 295.865478516\n",
      "Epoch 467 Loss:73.0910415649 Val loss: 298.031616211\n",
      "Epoch 468 Loss:75.3015213013 Val loss: 296.348114014\n",
      "Epoch 469 Loss:76.449005127 Val loss: 292.128082275\n",
      "Epoch 470 Loss:74.4499588013 Val loss: 290.511627197\n",
      "Epoch 471 Loss:77.9737701416 Val loss: 292.924407959\n",
      "Epoch 472 Loss:78.5721435547 Val loss: 293.79006958\n",
      "Epoch 473 Loss:78.7209396362 Val loss: 292.64743042\n",
      "Epoch 474 Loss:77.2914123535 Val loss: 295.430145264\n",
      "Epoch 475 Loss:75.8987045288 Val loss: 293.090148926\n",
      "Epoch 476 Loss:76.7825546265 Val loss: 291.493927002\n",
      "Epoch 477 Loss:75.9851455688 Val loss: 297.736206055\n",
      "Epoch 478 Loss:75.8644561768 Val loss: 298.367675781\n",
      "Epoch 479 Loss:74.7916259766 Val loss: 292.668151855\n",
      "Epoch 480 Loss:73.7761611938 Val loss: 298.158447266\n",
      "Epoch 481 Loss:77.6245346069 Val loss: 296.177032471\n",
      "Epoch 482 Loss:74.0081100464 Val loss: 292.014190674\n",
      "Epoch 483 Loss:72.7585067749 Val loss: 295.674194336\n",
      "Epoch 484 Loss:73.9552307129 Val loss: 299.598907471\n",
      "Epoch 485 Loss:75.3016586304 Val loss: 293.151031494\n",
      "Epoch 486 Loss:76.1339035034 Val loss: 297.976196289\n",
      "Epoch 487 Loss:74.1948242188 Val loss: 292.911468506\n",
      "Epoch 488 Loss:74.5668029785 Val loss: 291.858215332\n",
      "Epoch 489 Loss:73.075958252 Val loss: 289.312957764\n",
      "Epoch 490 Loss:73.8106079102 Val loss: 292.269378662\n",
      "Epoch 491 Loss:73.2199172974 Val loss: 294.716217041\n",
      "Epoch 492 Loss:77.2004241943 Val loss: 290.174194336\n",
      "Epoch 493 Loss:74.8839950562 Val loss: 289.354675293\n",
      "Epoch 494 Loss:72.6770629883 Val loss: 292.167877197\n",
      "Epoch 495 Loss:71.1997680664 Val loss: 294.725189209\n",
      "Epoch 496 Loss:72.6461105347 Val loss: 296.454284668\n",
      "Epoch 497 Loss:73.7304534912 Val loss: 300.99987793\n",
      "Epoch 498 Loss:74.7917709351 Val loss: 298.050445557\n",
      "Epoch 499 Loss:74.4294052124 Val loss: 292.035308838\n",
      "Epoch 500 Loss:74.4030838013 Val loss: 292.744354248\n",
      "Epoch 501 Loss:72.6073760986 Val loss: 292.364532471\n",
      "Epoch 502 Loss:73.0464630127 Val loss: 291.096313477\n",
      "Epoch 503 Loss:73.4750137329 Val loss: 288.727752686\n",
      "Epoch 504 Loss:71.8257141113 Val loss: 296.511108398\n",
      "Epoch 505 Loss:72.2109375 Val loss: 290.672821045\n",
      "Epoch 506 Loss:72.423614502 Val loss: 288.442504883\n",
      "Epoch 507 Loss:71.6100311279 Val loss: 293.726013184\n",
      "Epoch 508 Loss:77.6190109253 Val loss: 288.946807861\n",
      "Epoch 509 Loss:74.1711196899 Val loss: 293.844299316\n",
      "Epoch 510 Loss:69.1567764282 Val loss: 293.116149902\n",
      "Epoch 511 Loss:70.4334411621 Val loss: 294.741363525\n",
      "Epoch 512 Loss:71.33984375 Val loss: 288.838317871\n",
      "Epoch 513 Loss:70.3273925781 Val loss: 294.400177002\n",
      "Epoch 514 Loss:70.5054550171 Val loss: 293.718139648\n",
      "Epoch 515 Loss:72.6526565552 Val loss: 288.534332275\n",
      "Epoch 516 Loss:71.739730835 Val loss: 291.74899292\n",
      "Epoch 517 Loss:70.5248565674 Val loss: 297.77456665\n",
      "Epoch 518 Loss:68.9605789185 Val loss: 291.90447998\n",
      "Epoch 519 Loss:71.3020782471 Val loss: 294.578765869\n",
      "Epoch 520 Loss:71.9709396362 Val loss: 292.080200195\n",
      "Epoch 521 Loss:72.9310684204 Val loss: 288.998260498\n",
      "Epoch 522 Loss:74.0786972046 Val loss: 294.278320312\n",
      "Epoch 523 Loss:74.014541626 Val loss: 290.105438232\n",
      "Epoch 524 Loss:71.7914733887 Val loss: 291.093505859\n",
      "Epoch 525 Loss:70.9615707397 Val loss: 290.188781738\n",
      "Epoch 526 Loss:70.0812759399 Val loss: 292.902862549\n",
      "Epoch 527 Loss:69.9898376465 Val loss: 294.596588135\n",
      "Epoch 528 Loss:72.2434692383 Val loss: 291.558380127\n",
      "Epoch 529 Loss:72.6451568604 Val loss: 293.413482666\n",
      "Epoch 530 Loss:69.3986053467 Val loss: 297.613311768\n",
      "Epoch 531 Loss:69.1393203735 Val loss: 292.909057617\n",
      "Epoch 532 Loss:72.3606185913 Val loss: 294.492279053\n",
      "Epoch 533 Loss:70.6300430298 Val loss: 293.596801758\n",
      "Epoch 534 Loss:69.8295135498 Val loss: 293.825683594\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "fold_num = 1\n",
    "\n",
    "for train, test in kfold.split(encoded_X):\n",
    "   \n",
    "    print \"Fold {}\".format(fold_num)\n",
    "    \n",
    "    X_train, X_test = encoded_X[train], encoded_X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    # Create validation datasest\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)\n",
    "    \n",
    "    # Model\n",
    "    input_var = tf.placeholder(tf.float32, shape=[None, X_train.shape[1]])\n",
    "    gt_var = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    \n",
    "    model = tf.layers.dense(input_var, 50, activation=tf.nn.elu)\n",
    "    model = tf.layers.dense(model, 50, activation=tf.nn.elu)\n",
    "    model = tf.layers.dense(model, 50, activation=tf.nn.elu)\n",
    "    model = tf.layers.dense(model, 50, activation=tf.nn.elu)\n",
    "    model = tf.layers.dense(model, 50, activation=tf.nn.elu)\n",
    "    output = tf.layers.dense(model, 1) \n",
    "    \n",
    "    # Loss function \n",
    "    loss = tf.reduce_mean(tf.losses.mean_squared_error(gt_var, output)**.5 )\n",
    "    opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "  \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(1, EPOCHS+1):  \n",
    "            epoch_loss = []\n",
    "            for X_batch, y_batch in batch_iterator(X_train, y_train, BATCH_SIZE):\n",
    "                batch_loss, pred, _ = sess.run([loss, output, opt], feed_dict={input_var: X_batch, gt_var: y_batch})                      \n",
    "                epoch_loss.append(batch_loss)                                \n",
    "             # Validation\n",
    "            val_loss = sess.run(loss, feed_dict={input_var: X_val, gt_var: y_val})\n",
    "                                               \n",
    "            print \"Epoch {} Loss:{} Val loss: {}\".format(epoch, np.mean(epoch_loss), val_loss)\n",
    "        fold_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# scorer = make_scorer(lambda a, b: mean_squared_error(a, b)**.5)\n",
    "# scores = cross_val_score(lr, encoded_X, y, cv=5, scoring=scorer, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340.449211318938"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 284.83019248\n"
     ]
    }
   ],
   "source": [
    "y_pred = cbr.fit(X_train, y_train, cat_features=CATEGORICAL_TEST_FEATURES_IDX).predict(X_test)\n",
    "print \"Mean squared error: {}\".format(mean_squared_error(y_test, y_pred)**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core._CatBoostBase at 0x7f8a7f594a50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make submission\n",
    "cbr.fit(X, y, cat_features=CATEGORICAL_TEST_FEATURES_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test = preprocess(test, CATEGORICAL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = test[TEST_FEATURES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions = cbr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 879.78794459, 1165.70020084,  279.39788828, ...,  154.47198918,\n",
       "        -62.34725293,   36.55400176])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_submission(ids, predictions):\n",
    "    df = pd.concat([ids, pd.Series(predictions)], axis=1)\n",
    "    return df.rename(columns={0: 'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l = test.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([test.id, pd.Series(predictions)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={0: 'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"catboost_d10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}